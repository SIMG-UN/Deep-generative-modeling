{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "047e4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ef1e8",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "\n",
    " A tokenizer is a fundamental component in natural language processing that converts raw text into a sequence of tokens that can be processed by machine learning models. In the context of autoregressive language models, tokenization serves as the bridge between human-readable text and the numerical representations that neural networks can understand.\n",
    "\n",
    " ## Key Components of a Tokenizer:\n",
    " \n",
    " 1. **Vocabulary Building**: The process of creating a mapping between tokens (words, subwords, or characters) and unique numerical indices\n",
    " 2. **Text Preprocessing**: Cleaning and normalizing input text (lowercasing, handling punctuation, etc.)\n",
    " 3. **Tokenization Strategy**: Deciding how to split text into meaningful units\n",
    " 4. **Special Tokens**: Reserved tokens for specific purposes like unknown words (`<UNK>`), padding (`<PAD>`), start of sequence (`<SOS>`), and end of sequence (`<EOS>`)\n",
    "\n",
    "## Tokenization Approaches:\n",
    "\n",
    "- **Word-level**: Split text by whitespace and punctuation\n",
    "- **Subword-level**: Use algorithms like BPE (Byte Pair Encoding) or WordPiece\n",
    "- **Character-level**: Treat each character as a token\n",
    "\n",
    "The choice of tokenization strategy affects model performance, vocabulary size, and the ability to handle out-of-vocabulary words. Our simple tokenizer implementation uses word-level tokenization with special token handling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e58e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario creado con 915 tokens\n",
      "\n",
      "Texto original: En un lugar de la Mancha, de cuyo nombre no quiero acordarme\n",
      "Tokens codificados: [21, 22, 23, 9, 10, 19, 24, 9, 25, 26, 27, 28, 0]\n",
      "Texto decodificado: en un lugar de la mancha , de cuyo nombre no quiero <UNK>\n",
      "Tamaño del vocabulario: 915\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.idx_to_token = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def build_vocab(self, text, min_freq=1):\n",
    "        \"\"\"Construye el vocabulario a partir del texto\"\"\"\n",
    "        # Limpiar y tokenizar el texto\n",
    "        tokens = self._tokenize(text)\n",
    "        \n",
    "        # Contar frecuencias\n",
    "        token_counts = Counter(tokens)\n",
    "        \n",
    "        # Agregar tokens especiales\n",
    "        special_tokens = ['<UNK>', '<PAD>', '<SOS>', '<EOS>']\n",
    "        \n",
    "        # Crear vocabulario\n",
    "        self.vocab = {}\n",
    "        self.idx_to_token = {}\n",
    "        \n",
    "        # Agregar tokens especiales primero\n",
    "        for i, token in enumerate(special_tokens):\n",
    "            self.vocab[token] = i\n",
    "            self.idx_to_token[i] = token\n",
    "        \n",
    "        # Agregar tokens que cumplen con la frecuencia mínima\n",
    "        idx = len(special_tokens)\n",
    "        for token, count in token_counts.items():\n",
    "            if count >= min_freq:\n",
    "                self.vocab[token] = idx\n",
    "                self.idx_to_token[idx] = token\n",
    "                idx += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Vocabulario creado con {self.vocab_size} tokens\")\n",
    "        \n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Tokeniza el texto en palabras\"\"\"\n",
    "        # Convertir a minúsculas y dividir por espacios y puntuación\n",
    "        text = text.lower()\n",
    "        # Separar puntuación de las palabras\n",
    "        text = re.sub(r'([.!?,:;])', r' \\1 ', text)\n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convierte texto a índices\"\"\"\n",
    "        tokens = self._tokenize(text)\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                indices.append(self.vocab[token])\n",
    "            else:\n",
    "                indices.append(self.vocab['<UNK>'])\n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convierte índices a texto\"\"\"\n",
    "        tokens = []\n",
    "        for idx in indices:\n",
    "            if idx in self.idx_to_token:\n",
    "                tokens.append(self.idx_to_token[idx])\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce607412",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "362878e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario creado con 915 tokens\n",
      "\n",
      "Texto original: En un lugar de la Mancha, de cuyo nombre no quiero acordarme\n",
      "Tokens codificados: [21, 22, 23, 9, 10, 19, 24, 9, 25, 26, 27, 28, 0]\n",
      "Texto decodificado: en un lugar de la mancha , de cuyo nombre no quiero <UNK>\n",
      "Tamaño del vocabulario: 915\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/cervantes_2.txt', 'r', encoding='utf-8') as f:\n",
    "    texto_cervantes = f.read()\n",
    "\n",
    "# Crear y entrenar el tokenizador\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(texto_cervantes, min_freq=2)\n",
    "\n",
    "# Ejemplo de tokenización\n",
    "texto_ejemplo = \"En un lugar de la Mancha, de cuyo nombre no quiero acordarme\"\n",
    "tokens_encoded = tokenizer.encode(texto_ejemplo)\n",
    "texto_decoded = tokenizer.decode(tokens_encoded)\n",
    "\n",
    "print(f\"\\nTexto original: {texto_ejemplo}\")\n",
    "print(f\"Tokens codificados: {tokens_encoded}\")\n",
    "print(f\"Texto decodificado: {texto_decoded}\")\n",
    "print(f\"Tamaño del vocabulario: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef33522e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " '<PAD>': 1,\n",
       " '<SOS>': 2,\n",
       " '<EOS>': 3,\n",
       " 'primera': 4,\n",
       " 'parte': 5,\n",
       " 'capítulo': 6,\n",
       " 'que': 7,\n",
       " 'trata': 8,\n",
       " 'de': 9,\n",
       " 'la': 10,\n",
       " 'condición': 11,\n",
       " 'y': 12,\n",
       " 'ejercicio': 13,\n",
       " 'del': 14,\n",
       " 'famoso': 15,\n",
       " 'hidalgo': 16,\n",
       " 'don': 17,\n",
       " 'quijote': 18,\n",
       " 'mancha': 19,\n",
       " '.': 20,\n",
       " 'en': 21,\n",
       " 'un': 22,\n",
       " 'lugar': 23,\n",
       " ',': 24,\n",
       " 'cuyo': 25,\n",
       " 'nombre': 26,\n",
       " 'no': 27,\n",
       " 'quiero': 28,\n",
       " 'ha': 29,\n",
       " 'mucho': 30,\n",
       " 'tiempo': 31,\n",
       " 'vivía': 32,\n",
       " 'los': 33,\n",
       " 'lanza': 34,\n",
       " 'adarga': 35,\n",
       " 'rocín': 36,\n",
       " 'una': 37,\n",
       " 'algo': 38,\n",
       " 'más': 39,\n",
       " 'vaca': 40,\n",
       " 'las': 41,\n",
       " 'noches': 42,\n",
       " 'viernes': 43,\n",
       " 'algún': 44,\n",
       " 'añadidura': 45,\n",
       " 'tres': 46,\n",
       " 'partes': 47,\n",
       " 'su': 48,\n",
       " 'hacienda': 49,\n",
       " 'el': 50,\n",
       " 'della': 51,\n",
       " 'para': 52,\n",
       " 'con': 53,\n",
       " 'sus': 54,\n",
       " 'lo': 55,\n",
       " 'mesmo': 56,\n",
       " 'días': 57,\n",
       " 'se': 58,\n",
       " 'honraba': 59,\n",
       " 'tenía': 60,\n",
       " 'casa': 61,\n",
       " 'ama': 62,\n",
       " 'pasaba': 63,\n",
       " 'sobrina': 64,\n",
       " 'llegaba': 65,\n",
       " 'a': 66,\n",
       " 'campo': 67,\n",
       " 'así': 68,\n",
       " 'como': 69,\n",
       " 'edad': 70,\n",
       " 'nuestro': 71,\n",
       " 'años': 72,\n",
       " 'era': 73,\n",
       " 'seco': 74,\n",
       " 'rostro': 75,\n",
       " 'gran': 76,\n",
       " 'amigo': 77,\n",
       " 'caza': 78,\n",
       " 'decir': 79,\n",
       " 'sobrenombre': 80,\n",
       " '«quijada»': 81,\n",
       " 'o': 82,\n",
       " '«quesada»': 83,\n",
       " 'esto': 84,\n",
       " 'hay': 85,\n",
       " 'alguna': 86,\n",
       " 'autores': 87,\n",
       " 'deste': 88,\n",
       " 'caso': 89,\n",
       " 'aunque': 90,\n",
       " 'por': 91,\n",
       " 'entender': 92,\n",
       " 'llamaba': 93,\n",
       " 'pero': 94,\n",
       " 'poco': 95,\n",
       " ':': 96,\n",
       " 'basta': 97,\n",
       " 'narración': 98,\n",
       " 'dél': 99,\n",
       " 'salga': 100,\n",
       " 'punto': 101,\n",
       " 'verdad': 102,\n",
       " 'es': 103,\n",
       " 'pues': 104,\n",
       " 'saber': 105,\n",
       " 'este': 106,\n",
       " 'estaba': 107,\n",
       " '-que': 108,\n",
       " 'eran': 109,\n",
       " 'daba': 110,\n",
       " 'leer': 111,\n",
       " 'libros': 112,\n",
       " 'caballerías': 113,\n",
       " 'tanta': 114,\n",
       " 'afición': 115,\n",
       " 'gusto': 116,\n",
       " 'casi': 117,\n",
       " 'todo': 118,\n",
       " 'aun': 119,\n",
       " ';': 120,\n",
       " 'llegó': 121,\n",
       " 'tanto': 122,\n",
       " 'muchas': 123,\n",
       " 'tierra': 124,\n",
       " 'llevó': 125,\n",
       " 'todos': 126,\n",
       " 'cuantos': 127,\n",
       " 'pudo': 128,\n",
       " 'haber': 129,\n",
       " 'dellos': 130,\n",
       " 'le': 131,\n",
       " 'parecían': 132,\n",
       " 'tan': 133,\n",
       " 'bien': 134,\n",
       " 'compuso': 135,\n",
       " 'porque': 136,\n",
       " 'claridad': 137,\n",
       " 'aquellas': 138,\n",
       " 'razones': 139,\n",
       " 'suyas': 140,\n",
       " 'cuando': 141,\n",
       " 'aquellos': 142,\n",
       " 'requiebros': 143,\n",
       " 'desafíos': 144,\n",
       " 'donde': 145,\n",
       " 'hallaba': 146,\n",
       " 'escrito': 147,\n",
       " 'razón': 148,\n",
       " 'mi': 149,\n",
       " 'hace': 150,\n",
       " 'tal': 151,\n",
       " 'manera': 152,\n",
       " 'me': 153,\n",
       " 'vuestra': 154,\n",
       " 'también': 155,\n",
       " 'leía': 156,\n",
       " 'os': 157,\n",
       " 'grandeza': 158,\n",
       " 'estas': 159,\n",
       " 'pobre': 160,\n",
       " 'caballero': 161,\n",
       " 'juicio': 162,\n",
       " 'ni': 163,\n",
       " 'si': 164,\n",
       " 'solo': 165,\n",
       " 'ello': 166,\n",
       " 'muy': 167,\n",
       " 'heridas': 168,\n",
       " 'recebía': 169,\n",
       " 'imaginaba': 170,\n",
       " 'grandes': 171,\n",
       " 'hubiesen': 172,\n",
       " 'dejaría': 173,\n",
       " 'tener': 174,\n",
       " 'cuerpo': 175,\n",
       " 'autor': 176,\n",
       " 'aquel': 177,\n",
       " 'libro': 178,\n",
       " 'aquella': 179,\n",
       " 'aventura': 180,\n",
       " 'veces': 181,\n",
       " 'vino': 182,\n",
       " 'deseo': 183,\n",
       " 'tomar': 184,\n",
       " 'dalle': 185,\n",
       " 'fin': 186,\n",
       " 'al': 187,\n",
       " 'pie': 188,\n",
       " 'allí': 189,\n",
       " 'sin': 190,\n",
       " 'duda': 191,\n",
       " 'hiciera': 192,\n",
       " 'saliera': 193,\n",
       " 'otros': 194,\n",
       " 'mayores': 195,\n",
       " 'pensamientos': 196,\n",
       " 'tuvo': 197,\n",
       " 'cura': 198,\n",
       " 'hombre': 199,\n",
       " 'sobre': 200,\n",
       " 'cuál': 201,\n",
       " 'había': 202,\n",
       " 'sido': 203,\n",
       " 'mejor': 204,\n",
       " 'palmerín': 205,\n",
       " 'ingalaterra': 206,\n",
       " 'amadís': 207,\n",
       " 'gaula': 208,\n",
       " 'mas': 209,\n",
       " 'maese': 210,\n",
       " 'nicolás': 211,\n",
       " 'barbero': 212,\n",
       " 'pueblo': 213,\n",
       " 'decía': 214,\n",
       " 'ninguno': 215,\n",
       " 'alguno': 216,\n",
       " 'podía': 217,\n",
       " 'hermano': 218,\n",
       " 'iba': 219,\n",
       " 'él': 220,\n",
       " 'leyendo': 221,\n",
       " 'claro': 222,\n",
       " 'turbio': 223,\n",
       " 'dormir': 224,\n",
       " 'perder': 225,\n",
       " 'aquello': 226,\n",
       " 'amores': 227,\n",
       " 'disparates': 228,\n",
       " 'modo': 229,\n",
       " 'imaginación': 230,\n",
       " 'toda': 231,\n",
       " 'máquina': 232,\n",
       " 'soñadas': 233,\n",
       " 'otra': 234,\n",
       " 'historia': 235,\n",
       " 'mundo': 236,\n",
       " 'cid': 237,\n",
       " 'buen': 238,\n",
       " 'ver': 239,\n",
       " 'espada': 240,\n",
       " 'partido': 241,\n",
       " 'medio': 242,\n",
       " 'dos': 243,\n",
       " 'gigantes': 244,\n",
       " 'bernardo': 245,\n",
       " 'carpio': 246,\n",
       " 'roncesvalles': 247,\n",
       " 'muerto': 248,\n",
       " 'industria': 249,\n",
       " 'hijo': 250,\n",
       " 'entre': 251,\n",
       " 'gigante': 252,\n",
       " 'ser': 253,\n",
       " 'son': 254,\n",
       " 'criado': 255,\n",
       " 'reinaldos': 256,\n",
       " 'montalbán': 257,\n",
       " 'veía': 258,\n",
       " 'salir': 259,\n",
       " 'castillo': 260,\n",
       " 'mahoma': 261,\n",
       " 'según': 262,\n",
       " 'dice': 263,\n",
       " 'diera': 264,\n",
       " 'dar': 265,\n",
       " 'mano': 266,\n",
       " 'efeto': 267,\n",
       " 'ya': 268,\n",
       " 'estraño': 269,\n",
       " 'pensamiento': 270,\n",
       " 'jamás': 271,\n",
       " 'dio': 272,\n",
       " 'loco': 273,\n",
       " 'fue': 274,\n",
       " 'pareció': 275,\n",
       " 'honra': 276,\n",
       " 'servicio': 277,\n",
       " 'hacerse': 278,\n",
       " 'andante': 279,\n",
       " 'irse': 280,\n",
       " 'armas': 281,\n",
       " 'caballo': 282,\n",
       " 'buscar': 283,\n",
       " 'aventuras': 284,\n",
       " 'leído': 285,\n",
       " 'caballeros': 286,\n",
       " 'andantes': 287,\n",
       " 'deshaciendo': 288,\n",
       " 'género': 289,\n",
       " 'agravio': 290,\n",
       " 'ocasiones': 291,\n",
       " 'cobrase': 292,\n",
       " 'eterno': 293,\n",
       " 'fama': 294,\n",
       " 'valor': 295,\n",
       " 'brazo': 296,\n",
       " 'menos': 297,\n",
       " 'estos': 298,\n",
       " 'ellos': 299,\n",
       " 'sentía': 300,\n",
       " 'priesa': 301,\n",
       " 'poner': 302,\n",
       " 'deseaba': 303,\n",
       " 'primero': 304,\n",
       " 'hizo': 305,\n",
       " 'unas': 306,\n",
       " 'habían': 307,\n",
       " 'llenas': 308,\n",
       " 'estaban': 309,\n",
       " 'vio': 310,\n",
       " 'tenían': 311,\n",
       " 'falta': 312,\n",
       " 'celada': 313,\n",
       " 'encaje': 314,\n",
       " 'sino': 315,\n",
       " 'morrión': 316,\n",
       " 'fuerte': 317,\n",
       " 'estar': 318,\n",
       " 'sacó': 319,\n",
       " 'deshizo': 320,\n",
       " 'hecho': 321,\n",
       " 'dejó': 322,\n",
       " 'parecerle': 323,\n",
       " 'mal': 324,\n",
       " 'facilidad': 325,\n",
       " 'pedazos': 326,\n",
       " 'tornó': 327,\n",
       " 'hacer': 328,\n",
       " 'nuevo': 329,\n",
       " 'dentro': 330,\n",
       " 'quedó': 331,\n",
       " 'fortaleza': 332,\n",
       " 'nueva': 333,\n",
       " 'experiencia': 334,\n",
       " 'luego': 335,\n",
       " 'real': 336,\n",
       " 'cuatro': 337,\n",
       " 'qué': 338,\n",
       " 'sí': 339,\n",
       " 'bueno': 340,\n",
       " 'estuviese': 341,\n",
       " 'conocido': 342,\n",
       " 'ansí': 343,\n",
       " 'quién': 344,\n",
       " 'antes': 345,\n",
       " 'fuese': 346,\n",
       " 'puesto': 347,\n",
       " 'señor': 348,\n",
       " 'estado': 349,\n",
       " 'orden': 350,\n",
       " 'muchos': 351,\n",
       " 'quitó': 352,\n",
       " 'añadió': 353,\n",
       " 'memoria': 354,\n",
       " 'e': 355,\n",
       " 'llamar': 356,\n",
       " 'parecer': 357,\n",
       " 'significativo': 358,\n",
       " 'ahora': 359,\n",
       " 'quiso': 360,\n",
       " 'mismo': 361,\n",
       " 'ocho': 362,\n",
       " 'cabo': 363,\n",
       " '«don': 364,\n",
       " 'queda': 365,\n",
       " 'dicho': 366,\n",
       " 'ocasión': 367,\n",
       " 'desta': 368,\n",
       " 'verdadera': 369,\n",
       " 'debía': 370,\n",
       " 'valeroso': 371,\n",
       " 'llamarse': 372,\n",
       " 'patria': 373,\n",
       " 'hacerla': 374,\n",
       " 'llamó': 375,\n",
       " 'suyo': 376,\n",
       " 'suya': 377,\n",
       " 'linaje': 378,\n",
       " 'limpias': 379,\n",
       " 'cosa': 380,\n",
       " 'dama': 381,\n",
       " 'quien': 382,\n",
       " 'fruto': 383,\n",
       " '-si': 384,\n",
       " 'yo': 385,\n",
       " 'mis': 386,\n",
       " 'buena': 387,\n",
       " 'suerte': 388,\n",
       " 'encuentro': 389,\n",
       " 'ahí': 390,\n",
       " 'ordinario': 391,\n",
       " 'les': 392,\n",
       " 'mitad': 393,\n",
       " 'finalmente': 394,\n",
       " 'será': 395,\n",
       " 'rodillas': 396,\n",
       " 'ante': 397,\n",
       " 'dulce': 398,\n",
       " 'señora': 399,\n",
       " 'voz': 400,\n",
       " 'soy': 401,\n",
       " 'batalla': 402,\n",
       " 'debe': 403,\n",
       " 'cual': 404,\n",
       " 'mandó': 405,\n",
       " 'merced': 406,\n",
       " 'mí': 407,\n",
       " '?': 408,\n",
       " '¡oh': 409,\n",
       " 'cómo': 410,\n",
       " 'hubo': 411,\n",
       " 'halló': 412,\n",
       " '!': 413,\n",
       " 'cerca': 414,\n",
       " 'anduvo': 415,\n",
       " 'enamorado': 416,\n",
       " 'ella': 417,\n",
       " 'esta': 418,\n",
       " 'darle': 419,\n",
       " 'título': 420,\n",
       " 'buscándole': 421,\n",
       " 'princesa': 422,\n",
       " 'natural': 423,\n",
       " 'toboso': 424,\n",
       " 'demás': 425,\n",
       " 'cosas': 426,\n",
       " 'salida': 427,\n",
       " 'ingenioso': 428,\n",
       " 'hechas': 429,\n",
       " 'prevenciones': 430,\n",
       " 'pensaba': 431,\n",
       " 'hacía': 432,\n",
       " 'agravios': 433,\n",
       " 'tuertos': 434,\n",
       " 'sinrazones': 435,\n",
       " 'persona': 436,\n",
       " 'intención': 437,\n",
       " 'nadie': 438,\n",
       " 'viese': 439,\n",
       " 'mañana': 440,\n",
       " 'día': 441,\n",
       " 'uno': 442,\n",
       " 'mes': 443,\n",
       " 'todas': 444,\n",
       " 'subió': 445,\n",
       " 'rocinante': 446,\n",
       " 'puesta': 447,\n",
       " 'embrazó': 448,\n",
       " 'tomó': 449,\n",
       " 'puerta': 450,\n",
       " 'falsa': 451,\n",
       " 'corral': 452,\n",
       " 'salió': 453,\n",
       " 'contento': 454,\n",
       " 'dado': 455,\n",
       " 'principio': 456,\n",
       " 'apenas': 457,\n",
       " 'terrible': 458,\n",
       " 'dejar': 459,\n",
       " 'empresa': 460,\n",
       " 'armado': 461,\n",
       " 'ley': 462,\n",
       " 'caballería': 463,\n",
       " 'fuera': 464,\n",
       " 'llevar': 465,\n",
       " 'blancas': 466,\n",
       " 'novel': 467,\n",
       " 'hasta': 468,\n",
       " 'esfuerzo': 469,\n",
       " 'hicieron': 470,\n",
       " 'propósito': 471,\n",
       " 'locura': 472,\n",
       " 'armar': 473,\n",
       " 'fuesen': 474,\n",
       " 'camino': 475,\n",
       " 'otro': 476,\n",
       " 'quería': 477,\n",
       " 'consistía': 478,\n",
       " 'fuerza': 479,\n",
       " 'aventurero': 480,\n",
       " 'hablando': 481,\n",
       " 'consigo': 482,\n",
       " 'diciendo': 483,\n",
       " '-¿quién': 484,\n",
       " 'luz': 485,\n",
       " 'famosos': 486,\n",
       " 'hechos': 487,\n",
       " 'sabio': 488,\n",
       " 'llegue': 489,\n",
       " 'contar': 490,\n",
       " 'tendido': 491,\n",
       " 'pequeños': 492,\n",
       " 'venida': 493,\n",
       " 'dejando': 494,\n",
       " 'cama': 495,\n",
       " 'mostraba': 496,\n",
       " 'comenzó': 497,\n",
       " 'caminar': 498,\n",
       " 'caminaba': 499,\n",
       " 'adonde': 500,\n",
       " 'hazañas': 501,\n",
       " 'mías': 502,\n",
       " 'tú': 503,\n",
       " 'encantador': 504,\n",
       " 'quienquiera': 505,\n",
       " 'seas': 506,\n",
       " 'tocar': 507,\n",
       " 'te': 508,\n",
       " 'mío': 509,\n",
       " '-¡oh': 510,\n",
       " 'dulcinea': 511,\n",
       " 'cautivo': 512,\n",
       " 'corazón': 513,\n",
       " 'fermosura': 514,\n",
       " 'vuestro': 515,\n",
       " 'amor': 516,\n",
       " 'cuanto': 517,\n",
       " 'lenguaje': 518,\n",
       " 'sol': 519,\n",
       " 'algunos': 520,\n",
       " 'tuviera': 521,\n",
       " 'quisiera': 522,\n",
       " 'dicen': 523,\n",
       " 'he': 524,\n",
       " 'hallado': 525,\n",
       " 'hallaron': 526,\n",
       " 'hambre': 527,\n",
       " 'pudiese': 528,\n",
       " 'mucha': 529,\n",
       " 'necesidad': 530,\n",
       " 'lejos': 531,\n",
       " 'venta': 532,\n",
       " 'anochecía': 533,\n",
       " 'acaso': 534,\n",
       " 'mozas': 535,\n",
       " 'destas': 536,\n",
       " 'llaman': 537,\n",
       " 'cuales': 538,\n",
       " 'iban': 539,\n",
       " 'sevilla': 540,\n",
       " 'unos': 541,\n",
       " 'arrieros': 542,\n",
       " 'noche': 543,\n",
       " 'parecía': 544,\n",
       " 'pasar': 545,\n",
       " 'representó': 546,\n",
       " 'torres': 547,\n",
       " 'semejantes': 548,\n",
       " 'llegando': 549,\n",
       " 'trecho': 550,\n",
       " 'riendas': 551,\n",
       " 'esperando': 552,\n",
       " 'enano': 553,\n",
       " 'pusiese': 554,\n",
       " 'señal': 555,\n",
       " 'llegar': 556,\n",
       " 'caballeriza': 557,\n",
       " 'parecieron': 558,\n",
       " 'doncellas': 559,\n",
       " 'damas': 560,\n",
       " 'delante': 561,\n",
       " 'sucedió': 562,\n",
       " 'andaba': 563,\n",
       " 'recogiendo': 564,\n",
       " 'manada': 565,\n",
       " 'puercos': 566,\n",
       " '(que': 567,\n",
       " 'vieron': 568,\n",
       " 'miedo': 569,\n",
       " 'visera': 570,\n",
       " 'gentil': 571,\n",
       " 'talante': 572,\n",
       " 'dijo': 573,\n",
       " 'vuestras': 574,\n",
       " 'mercedes': 575,\n",
       " 'non': 576,\n",
       " 'ojos': 577,\n",
       " 'mala': 578,\n",
       " 'profesión': 579,\n",
       " 'pudieron': 580,\n",
       " 'risa': 581,\n",
       " '-bien': 582,\n",
       " 'parece': 583,\n",
       " 'sandez': 584,\n",
       " 'causa': 585,\n",
       " 'vos': 586,\n",
       " 'digo': 587,\n",
       " 'serviros': 588,\n",
       " 'señoras': 589,\n",
       " 'talle': 590,\n",
       " 'ellas': 591,\n",
       " 'adelante': 592,\n",
       " 'ventero': 593,\n",
       " 'viendo': 594,\n",
       " 'figura': 595,\n",
       " 'contrahecha': 596,\n",
       " 'nada': 597,\n",
       " 'muestras': 598,\n",
       " 'tantos': 599,\n",
       " 'determinó': 600,\n",
       " 'posada': 601,\n",
       " 'lecho': 602,\n",
       " 'humildad': 603,\n",
       " 'alcaide': 604,\n",
       " 'respondió': 605,\n",
       " 'castellano': 606,\n",
       " 'cualquiera': 607,\n",
       " 'huésped': 608,\n",
       " 'haberle': 609,\n",
       " 'llamado': 610,\n",
       " 'parecido': 611,\n",
       " 'sanos': 612,\n",
       " 'castilla': 613,\n",
       " 'playa': 614,\n",
       " 'sanlúcar': 615,\n",
       " 'caco': 616,\n",
       " 'eso': 617,\n",
       " 'siempre': 618,\n",
       " 'velar': 619,\n",
       " 'siendo': 620,\n",
       " 'puede': 621,\n",
       " 'hallar': 622,\n",
       " 'año': 623,\n",
       " 'trabajo': 624,\n",
       " 'tuviese': 625,\n",
       " 'cuidado': 626,\n",
       " 'pieza': 627,\n",
       " 'pan': 628,\n",
       " 'volvió': 629,\n",
       " 'peto': 630,\n",
       " 'espaldar': 631,\n",
       " 'traía': 632,\n",
       " 'atada': 633,\n",
       " 'cintas': 634,\n",
       " 'menester': 635,\n",
       " 'quitar': 636,\n",
       " 'ninguna': 637,\n",
       " 'graciosa': 638,\n",
       " 'pensar': 639,\n",
       " 'algunas': 640,\n",
       " 'principales': 641,\n",
       " 'servido': 642,\n",
       " 'aldea': 643,\n",
       " 'fasta': 644,\n",
       " 'fazañas': 645,\n",
       " 'pro': 646,\n",
       " 'romance': 647,\n",
       " 'tengo': 648,\n",
       " 'oír': 649,\n",
       " 'retóricas': 650,\n",
       " 'palabra': 651,\n",
       " 'comer': 652,\n",
       " '-respondió': 653,\n",
       " 'quijote-': 654,\n",
       " 'entiendo': 655,\n",
       " 'acertó': 656,\n",
       " 'pescado': 657,\n",
       " 'abadejo': 658,\n",
       " 'bacallao': 659,\n",
       " 'otras': 660,\n",
       " 'truchuela': 661,\n",
       " 'ventura': 662,\n",
       " 'truchuelas': 663,\n",
       " 'da': 664,\n",
       " 'den': 665,\n",
       " 'reales': 666,\n",
       " 'podría': 667,\n",
       " 'fuere': 668,\n",
       " 'venga': 669,\n",
       " 'trújole': 670,\n",
       " 'grande': 671,\n",
       " 'verle': 672,\n",
       " 'boca': 673,\n",
       " 'manos': 674,\n",
       " 'ponía': 675,\n",
       " 'posible': 676,\n",
       " 'paciencia': 677,\n",
       " 'trueco': 678,\n",
       " 'estando': 679,\n",
       " 'acabó': 680,\n",
       " 'verse': 681,\n",
       " 'recebir': 682,\n",
       " 'cuenta': 683,\n",
       " '-no': 684,\n",
       " 'estoy': 685,\n",
       " 'pedirle': 686,\n",
       " 'pies': 687,\n",
       " 'pedía': 688,\n",
       " 'esperaba': 689,\n",
       " 'habéis': 690,\n",
       " 'capilla': 691,\n",
       " 'poder': 692,\n",
       " 'ir': 693,\n",
       " 'buscando': 694,\n",
       " 'está': 695,\n",
       " 'acertado': 696,\n",
       " 'diversas': 697,\n",
       " 'hubiese': 698,\n",
       " 'toledo': 699,\n",
       " 'haciendo': 700,\n",
       " 'conocer': 701,\n",
       " 'españa': 702,\n",
       " 'pago': 703,\n",
       " 'díjole': 704,\n",
       " 'dondequiera': 705,\n",
       " 'patio': 706,\n",
       " 'dios': 707,\n",
       " 'ceremonias': 708,\n",
       " 'preguntóle': 709,\n",
       " 'dineros': 710,\n",
       " 'nunca': 711,\n",
       " 'historias': 712,\n",
       " 'traído': 713,\n",
       " 'dellas': 714,\n",
       " 'necesaria': 715,\n",
       " 'camisas': 716,\n",
       " 'llevaban': 717,\n",
       " 'llena': 718,\n",
       " 'ungüentos': 719,\n",
       " 'curar': 720,\n",
       " 'salían': 721,\n",
       " 'heridos': 722,\n",
       " 'agua': 723,\n",
       " 'quedaban': 724,\n",
       " 'acertada': 725,\n",
       " 'escuderos': 726,\n",
       " 'necesarias': 727,\n",
       " 'curarse': 728,\n",
       " 'tales': 729,\n",
       " 'alforjas': 730,\n",
       " 'consejo': 731,\n",
       " 'presto': 732,\n",
       " 'lado': 733,\n",
       " 'pila': 734,\n",
       " 'junto': 735,\n",
       " 'pozo': 736,\n",
       " 'cerrar': 737,\n",
       " 'contó': 738,\n",
       " 'vela': 739,\n",
       " 'desde': 740,\n",
       " 'sosegado': 741,\n",
       " 'espacio': 742,\n",
       " 'visto': 743,\n",
       " 'vida': 744,\n",
       " 'tu': 745,\n",
       " 'atrevimiento': 746,\n",
       " 'arriero': 747,\n",
       " 'curara': 748,\n",
       " 'alzó': 749,\n",
       " 'cielo': 750,\n",
       " 'mía': 751,\n",
       " 'favor': 752,\n",
       " 'golpe': 753,\n",
       " 'cabeza': 754,\n",
       " 'suelo': 755,\n",
       " 'recogió': 756,\n",
       " 'pasado': 757,\n",
       " 'aún': 758,\n",
       " 'mesma': 759,\n",
       " 'vez': 760,\n",
       " 'gente': 761,\n",
       " 'debilitado': 762,\n",
       " 'compañeros': 763,\n",
       " 'voces': 764,\n",
       " 'dejasen': 765,\n",
       " 'hubiera': 766,\n",
       " 'recebido': 767,\n",
       " 'vosotros': 768,\n",
       " 'baja': 769,\n",
       " 'hago': 770,\n",
       " 'desgracia': 771,\n",
       " 'supiese': 772,\n",
       " 'quedar': 773,\n",
       " 'espaldarazo': 774,\n",
       " 'creyó': 775,\n",
       " 'viva': 776,\n",
       " 'respeto': 777,\n",
       " 'medroso': 778,\n",
       " 'muchacho': 779,\n",
       " 'cada': 780,\n",
       " 'haga': 781,\n",
       " 'dé': 782,\n",
       " 'preguntó': 783,\n",
       " 'quedaba': 784,\n",
       " 'obligado': 785,\n",
       " 'hija': 786,\n",
       " 'llamase': 787,\n",
       " '«doña': 788,\n",
       " 'honrado': 789,\n",
       " 'antequera': 790,\n",
       " 'hora': 791,\n",
       " 'palabras': 792,\n",
       " 'volver': 793,\n",
       " 'labrador': 794,\n",
       " 'vecino': 795,\n",
       " 'hacia': 796,\n",
       " 'gana': 797,\n",
       " 'bosque': 798,\n",
       " 'quejaba': 799,\n",
       " 'oído': 800,\n",
       " 'doy': 801,\n",
       " 'pueda': 802,\n",
       " 'cumplir': 803,\n",
       " 'debo': 804,\n",
       " 'encaminó': 805,\n",
       " 'entró': 806,\n",
       " 'encina': 807,\n",
       " 'dando': 808,\n",
       " 'azotes': 809,\n",
       " 'lengua': 810,\n",
       " 'respondía': 811,\n",
       " 'haré': 812,\n",
       " 'aquí': 813,\n",
       " 'tomad': 814,\n",
       " '-señor': 815,\n",
       " 'castigo': 816,\n",
       " 'soldada': 817,\n",
       " 'villano': 818,\n",
       " '-dijo': 819,\n",
       " 'nos': 820,\n",
       " 'réplica': 821,\n",
       " 'responder': 822,\n",
       " 'amo': 823,\n",
       " 'nueve': 824,\n",
       " 'morir': 825,\n",
       " 'paso': 826,\n",
       " 'juramento': 827,\n",
       " '-y': 828,\n",
       " 'jurado': 829,\n",
       " 'pares': 830,\n",
       " 'zapatos': 831,\n",
       " 'sangrías': 832,\n",
       " 'enfermo': 833,\n",
       " '-replicó': 834,\n",
       " 'culpa': 835,\n",
       " 'sangre': 836,\n",
       " '-el': 837,\n",
       " 'daño': 838,\n",
       " 'andrés': 839,\n",
       " 'conmigo': 840,\n",
       " 'muchacho-': 841,\n",
       " 'hará': 842,\n",
       " 'paga': 843,\n",
       " '-mire': 844,\n",
       " 'obras': 845,\n",
       " '-así': 846,\n",
       " 'andrés-': 847,\n",
       " 'sudor': 848,\n",
       " 'labrador-': 849,\n",
       " 'juro': 850,\n",
       " 'desfacedor': 851,\n",
       " 'pena': 852,\n",
       " 'volvióse': 853,\n",
       " 'mandado': 854,\n",
       " 'mil': 855,\n",
       " 'acrecentar': 856,\n",
       " 'acordó': 857,\n",
       " 'valdovinos': 858,\n",
       " 'marqués': 859,\n",
       " 'mantua': 860,\n",
       " 'herido': 861,\n",
       " 'venía': 862,\n",
       " 'tío': 863,\n",
       " 'verso': 864,\n",
       " 'oyendo': 865,\n",
       " 'conoció': 866,\n",
       " 'quijana': 867,\n",
       " 'jumento': 868,\n",
       " 'molido': 869,\n",
       " 'diablo': 870,\n",
       " 'moro': 871,\n",
       " 'abindarráez': 872,\n",
       " 'rodrigo': 873,\n",
       " 'narváez': 874,\n",
       " 'larga': 875,\n",
       " '-sepa': 876,\n",
       " 'han': 877,\n",
       " 'sé': 878,\n",
       " 'doce': 879,\n",
       " 'francia': 880,\n",
       " 'amigos': 881,\n",
       " 'licenciado': 882,\n",
       " 'cura-': 883,\n",
       " 'tiene': 884,\n",
       " 'suele': 885,\n",
       " 'acuerdo': 886,\n",
       " 'sean': 887,\n",
       " 'feridas': 888,\n",
       " 'fuego': 889,\n",
       " 'viene': 890,\n",
       " 'malferido': 891,\n",
       " 'ama-': 892,\n",
       " 'esa': 893,\n",
       " 'aposento': 894,\n",
       " 'tienen': 895,\n",
       " 'perdonar': 896,\n",
       " 'hoguera': 897,\n",
       " 'siquiera': 898,\n",
       " 'barbero-': 899,\n",
       " 'esotro': 900,\n",
       " 'esplandián': 901,\n",
       " '-pues': 902,\n",
       " 'padre': 903,\n",
       " 'ventana': 904,\n",
       " '-este': 905,\n",
       " 'ese': 906,\n",
       " 'verdadero': 907,\n",
       " 'florismarte': 908,\n",
       " 'nacimiento': 909,\n",
       " 'hallo': 910,\n",
       " 'cruz': 911,\n",
       " 'anda': 912,\n",
       " 'cristiano': 913,\n",
       " 'habla': 914}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3b71023",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(texto_cervantes) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de862a",
   "metadata": {},
   "source": [
    "# Next token dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8baf0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextTokenDataset(Dataset):\n",
    "    def __init__(self, tokens, context_size=5):\n",
    "        self.tokens = tokens\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.context_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.tokens[idx: idx + self.context_size]\n",
    "        y = self.tokens[idx + self.context_size]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcdeae4",
   "metadata": {},
   "source": [
    "## Autoregressive Models\n",
    "\n",
    "Autoregressive models form a fundamental class of probabilistic models that predict future values in a sequence based on previous observations. The core principle lies in modeling the conditional probability distribution of each element given its predecessors.\n",
    "\n",
    "Mathematically, for a sequence $x_1, x_2, \\ldots, x_T$, an autoregressive model decomposes the joint probability using the chain rule:\n",
    "\n",
    "$$P(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^{T} P(x_t | x_1, x_2, \\ldots, x_{t-1})$$\n",
    "\n",
    "In practice, we often limit the dependency to a finite context window of size $k$, leading to:\n",
    "\n",
    "$$P(x_t | x_1, \\ldots, x_{t-1}) \\approx P(x_t | x_{t-k}, \\ldots, x_{t-1})$$\n",
    "\n",
    "This approximation makes the model computationally tractable while maintaining the sequential dependency structure. For language modeling, each $x_t$ represents a token (word, subword, or character), and the model learns to estimate the probability distribution over the vocabulary given the preceding context.\n",
    "\n",
    "The autoregressive formulation naturally handles variable-length sequences and provides a principled approach to text generation through sequential sampling. During inference, we can generate new sequences by iteratively sampling from the predicted distributions and appending the sampled tokens to the context for subsequent predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7921259",
   "metadata": {},
   "source": [
    "## Autoregressive Models with Finite Memory\n",
    "\n",
    "Autoregressive models with finite memory represent a practical approximation where we limit the contextual dependency to a fixed window of previous tokens. Instead of considering the entire preceding sequence, these models use only the last $k$ tokens to predict the next element.\n",
    "\n",
    "For a context of size $k$, the model learns the function:\n",
    "\n",
    "$$P(x_t | x_{t-k}, x_{t-k+1}, \\ldots, x_{t-1})$$\n",
    "\n",
    " ### MLP Implementation\n",
    " \n",
    " A direct approach to implement this architecture consists of using a Multi-Layer Perceptron (MLP) that takes a fixed context window and predicts the next token. The implementation follows these key components:\n",
    " \n",
    " 1. **Embeddings**: Converts each input token into a dense vector of fixed dimension using `nn.Embedding`\n",
    " 2. **Concatenation**: Flattens the context embeddings into a single vector by reshaping from `(batch, context_size, embed_dim)` to `(batch, context_size * embed_dim)`\n",
    " 3. **Dense layers**: Processes the concatenated representation through linear transformations with ReLU activations\n",
    " 4. **Output**: Produces logits over the entire vocabulary for the next token prediction\n",
    " \n",
    " The `NextTokenDataset` class handles the data preparation by creating input-target pairs where:\n",
    " - Input `x`: A sequence of `context_size` tokens \n",
    " - Target `y`: The next token following the context window\n",
    " \n",
    " This architecture is computationally efficient and allows for fast training, although it lacks the ability to model long-range dependencies that more sophisticated architectures like Transformers possess. The finite memory constraint makes it particularly suitable for tasks where local context is sufficient for accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21ba97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# tokens = [ya tienes la lista de enteros de tu corpus]\n",
    "context_size = 2\n",
    "dataset = NextTokenDataset(encoded_text, context_size)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b51a7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleAutoregressiveModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, context_size=5, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim * context_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, context_size)\n",
    "        embeds = self.embedding(x)   # (batch, context_size, embed_dim)\n",
    "        flat = embeds.view(embeds.size(0), -1)  # flatten\n",
    "        h = F.relu(self.fc1(flat))\n",
    "        out = self.fc2(h)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "faea62d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.0309\n",
      "Epoch 2, Loss: 4.2153\n",
      "Epoch 3, Loss: 3.8170\n",
      "Epoch 4, Loss: 3.4639\n",
      "Epoch 5, Loss: 3.1544\n"
     ]
    }
   ],
   "source": [
    "vocab_size = max(encoded_text) + 1  \n",
    "model = SimpleAutoregressiveModel(vocab_size, embed_dim=64, context_size=context_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e3daa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate(model, seed_tokens, length=20):\n",
    "    model.eval()\n",
    "    context = seed_tokens[-context_size:]  # mantener tamaño de contexto\n",
    "    generated = seed_tokens[:]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        x = torch.tensor([context], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(next_token)\n",
    "        context = generated[-context_size:]\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40305d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 22, 23, 9, 10, 19]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_text = \"En un lugar de la Mancha\"\n",
    "encoded_base_text = tokenizer.encode(base_text)\n",
    "encoded_base_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a89a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "def generate(model, seed_tokens, length=20):\n",
    "    model.eval()\n",
    "    context = seed_tokens[-context_size:]\n",
    "    generated = seed_tokens[:]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        x = torch.tensor([context], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(next_token)\n",
    "        context = generated[-context_size:]\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65b0e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(model, encoded_base_text, length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c571d51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 22, 23, 9, 10, 19, 7, 24, 494, 84, 7, 66, 678, 9, 12, 243, 126, 33, 21, 102, 819, 50, 883, 24, 190, 7]\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07173e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en un lugar de la mancha que , dejando esto que a trueco de y dos todos los en verdad -dijo el cura- , sin que'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856b049",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs) for Autoregressive Modeling\n",
    "\n",
    "Recurrent Neural Networks represent a fundamental architecture for sequential data modeling, particularly well-suited for autoregressive language modeling tasks. Unlike feedforward networks that process fixed-size contexts, RNNs maintain an internal hidden state that theoretically allows them to capture dependencies of arbitrary length in sequences.\n",
    "\n",
    "### RNN Architecture for Language Modeling\n",
    "\n",
    "In the context of autoregressive modeling, RNNs process sequences token by token, updating their hidden state at each time step. The hidden state serves as a compressed representation of all previously seen tokens, enabling the model to make predictions based on the entire sequence history rather than just a fixed context window.\n",
    "\n",
    "The fundamental equations governing an RNN are:\n",
    " - $h_t = \\tanh(W_{hh} \\cdot h_{t-1} + W_{xh} \\cdot x_t + b_h)$\n",
    "  - $y_t = W_{hy} \\cdot h_t + b_y$\n",
    "\n",
    "Where h_t represents the hidden state at time t, x_t is the input token embedding, and y_t is the output logits for the vocabulary distribution.\n",
    "\n",
    "### Advantages in Autoregressive Settings\n",
    "\n",
    "RNNs offer several benefits for autoregressive language modeling:\n",
    "1. **Variable-length context**: Unlike n-gram models or fixed-context transformers, RNNs can theoretically capture dependencies from the beginning of a sequence\n",
    "2. **Parameter efficiency**: The same set of parameters is reused at each time step, making RNNs memory-efficient compared to architectures that explicitly model all pairwise interactions\n",
    "3. **Sequential processing**: The inherently sequential nature of RNNs aligns well with the left-to-right generation process in autoregressive modeling\n",
    "\n",
    "### Limitations and Practical Considerations\n",
    "\n",
    "Despite their theoretical advantages, vanilla RNNs face significant challenges in practice:\n",
    "- **Vanishing gradients**: Long-range dependencies become difficult to learn due to exponential decay of gradients through time\n",
    "- **Sequential computation**: Unlike transformer architectures, RNNs cannot be easily parallelized during training\n",
    "- **Limited context retention**: In practice, RNNs struggle to maintain information over very long sequences\n",
    "\n",
    "These limitations led to the development of more sophisticated variants like LSTMs and GRUs, and eventually to the adoption of attention-based architectures for many language modeling applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc71ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: (batch, context_size)\n",
    "        embeds = self.embedding(x)  # (batch, context_size, embed_dim)\n",
    "        out, hidden = self.rnn(embeds, hidden)  # out: (batch, context_size, hidden_dim)\n",
    "        out = self.fc(out[:, -1, :])  # usamos solo la última salida\n",
    "        return out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbace975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.0181\n",
      "Epoch 2, Loss: 4.4617\n",
      "Epoch 3, Loss: 4.1616\n",
      "Epoch 4, Loss: 3.9130\n",
      "Epoch 5, Loss: 3.6853\n",
      "Epoch 6, Loss: 3.4756\n",
      "Epoch 7, Loss: 3.2708\n",
      "Epoch 8, Loss: 3.0828\n",
      "Epoch 9, Loss: 2.8962\n",
      "Epoch 10, Loss: 2.7176\n",
      "Epoch 11, Loss: 2.5450\n",
      "Epoch 12, Loss: 2.3800\n",
      "Epoch 13, Loss: 2.2220\n",
      "Epoch 14, Loss: 2.0697\n",
      "Epoch 15, Loss: 1.9213\n",
      "Epoch 16, Loss: 1.7826\n",
      "Epoch 17, Loss: 1.6502\n",
      "Epoch 18, Loss: 1.5247\n",
      "Epoch 19, Loss: 1.4063\n",
      "Epoch 20, Loss: 1.2897\n",
      "Epoch 21, Loss: 1.1828\n",
      "Epoch 22, Loss: 1.0859\n",
      "Epoch 23, Loss: 0.9882\n",
      "Epoch 24, Loss: 0.9035\n",
      "Epoch 25, Loss: 0.8191\n",
      "Epoch 26, Loss: 0.7398\n",
      "Epoch 27, Loss: 0.6683\n",
      "Epoch 28, Loss: 0.6066\n",
      "Epoch 29, Loss: 0.5454\n",
      "Epoch 30, Loss: 0.4937\n",
      "Epoch 31, Loss: 0.4479\n",
      "Epoch 32, Loss: 0.4059\n",
      "Epoch 33, Loss: 0.3667\n",
      "Epoch 34, Loss: 0.3321\n",
      "Epoch 35, Loss: 0.2954\n",
      "Epoch 36, Loss: 0.2681\n",
      "Epoch 37, Loss: 0.2534\n",
      "Epoch 38, Loss: 0.2240\n",
      "Epoch 39, Loss: 0.2030\n",
      "Epoch 40, Loss: 0.2178\n",
      "Epoch 41, Loss: 0.1926\n",
      "Epoch 42, Loss: 0.1719\n",
      "Epoch 43, Loss: 0.1487\n",
      "Epoch 44, Loss: 0.1243\n",
      "Epoch 45, Loss: 0.1166\n",
      "Epoch 46, Loss: 0.1360\n",
      "Epoch 47, Loss: 0.2486\n",
      "Epoch 48, Loss: 0.2440\n",
      "Epoch 49, Loss: 0.1222\n",
      "Epoch 50, Loss: 0.0668\n",
      "Epoch 51, Loss: 0.0443\n",
      "Epoch 52, Loss: 0.0354\n",
      "Epoch 53, Loss: 0.0296\n",
      "Epoch 54, Loss: 0.0266\n",
      "Epoch 55, Loss: 0.0242\n",
      "Epoch 56, Loss: 0.0260\n",
      "Epoch 57, Loss: 0.7544\n",
      "Epoch 58, Loss: 0.7257\n",
      "Epoch 59, Loss: 0.3153\n",
      "Epoch 60, Loss: 0.1582\n",
      "Epoch 61, Loss: 0.0923\n",
      "Epoch 62, Loss: 0.0541\n",
      "Epoch 63, Loss: 0.0388\n",
      "Epoch 64, Loss: 0.0305\n",
      "Epoch 65, Loss: 0.0255\n",
      "Epoch 66, Loss: 0.0224\n",
      "Epoch 67, Loss: 0.0195\n",
      "Epoch 68, Loss: 0.0174\n",
      "Epoch 69, Loss: 0.0165\n",
      "Epoch 70, Loss: 0.0189\n",
      "Epoch 71, Loss: 1.2267\n",
      "Epoch 72, Loss: 0.5769\n",
      "Epoch 73, Loss: 0.2882\n",
      "Epoch 74, Loss: 0.1496\n",
      "Epoch 75, Loss: 0.0899\n",
      "Epoch 76, Loss: 0.0587\n",
      "Epoch 77, Loss: 0.0432\n",
      "Epoch 78, Loss: 0.0321\n",
      "Epoch 79, Loss: 0.0251\n",
      "Epoch 80, Loss: 0.0207\n",
      "Epoch 81, Loss: 0.0177\n",
      "Epoch 82, Loss: 0.0157\n",
      "Epoch 83, Loss: 0.0140\n",
      "Epoch 84, Loss: 0.4374\n",
      "Epoch 85, Loss: 1.0151\n",
      "Epoch 86, Loss: 0.4487\n",
      "Epoch 87, Loss: 0.2326\n",
      "Epoch 88, Loss: 0.1317\n",
      "Epoch 89, Loss: 0.0765\n",
      "Epoch 90, Loss: 0.0486\n",
      "Epoch 91, Loss: 0.0339\n",
      "Epoch 92, Loss: 0.0250\n",
      "Epoch 93, Loss: 0.0211\n",
      "Epoch 94, Loss: 0.0180\n",
      "Epoch 95, Loss: 0.0157\n",
      "Epoch 96, Loss: 0.0137\n",
      "Epoch 97, Loss: 0.0122\n",
      "Epoch 98, Loss: 0.0660\n",
      "Epoch 99, Loss: 1.1232\n",
      "Epoch 100, Loss: 0.5155\n"
     ]
    }
   ],
   "source": [
    "context_size = 10\n",
    "dataset = NextTokenDataset(encoded_text, context_size)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "vocab_size = max(encoded_text) + 1\n",
    "model = RNNLanguageModel(vocab_size, embed_dim=64, hidden_dim=128)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits, hidden = model(x, hidden=None)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99543049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, seed_tokens, length=20):\n",
    "    model.eval()\n",
    "    context = seed_tokens[-context_size:]\n",
    "    generated = seed_tokens[:]\n",
    "    hidden = None\n",
    "    \n",
    "    for _ in range(length):\n",
    "        x = torch.tensor([context], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            logits, hidden = model(x, hidden)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(next_token)\n",
    "        context = generated[-context_size:]\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2eaf4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en un lugar de la mancha , de cuyo nombre no quiero <UNK> , rocín <UNK> a las armas tan <UNK> , y a decir que'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = generate(model, encoded_base_text, length=20)\n",
    "generated_text \n",
    "tokenizer.decode(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e7b66",
   "metadata": {},
   "source": [
    "## LSTM Networks in Autoregressive Models\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks represent a significant advancement in recurrent neural network architectures, specifically designed to address the vanishing gradient problem that plagues traditional RNNs. In the context of autoregressive modeling, LSTMs provide a more sophisticated mechanism for capturing long-range dependencies in sequential data.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The LSTM architecture introduces three gating mechanisms that control information flow through the cell state:\n",
    "\n",
    "**Forget Gate**: Determines what information to discard from the cell state\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "**Input Gate**: Controls which values to update in the cell state\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "**Output Gate**: Determines what parts of the cell state to output\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "The cell state and hidden state updates follow:\n",
    "$$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
    "$$h_t = o_t * \\tanh(C_t)$$\n",
    "\n",
    "Where $\\sigma$ represents the sigmoid function, $W$ are weight matrices, $b$ are bias vectors, and $*$ denotes element-wise multiplication.\n",
    "\n",
    "### Advantages in Autoregressive Modeling\n",
    "\n",
    "1. **Long-term Dependencies**: The cell state acts as a \"memory highway\" that can preserve information across many time steps, enabling the model to capture dependencies spanning longer sequences than traditional RNNs.\n",
    "\n",
    "2. **Gradient Flow**: The additive nature of cell state updates helps mitigate the vanishing gradient problem, allowing for more stable training on longer sequences.\n",
    "\n",
    "3. **Selective Memory**: The gating mechanisms provide the model with learnable control over what information to remember, forget, or output, making it particularly effective for language modeling tasks.\n",
    "\n",
    "4. **Contextual Understanding**: In autoregressive text generation, LSTMs can maintain context about previously generated tokens while selectively incorporating new information.\n",
    "\n",
    "### Limitations and Drawbacks\n",
    "\n",
    "1. **Computational Complexity**: LSTMs are significantly more computationally expensive than simple RNNs due to the multiple gate operations and matrix multiplications required at each time step.\n",
    "\n",
    "2. **Sequential Processing**: The inherently sequential nature of LSTMs prevents parallelization across time steps during training, limiting scalability compared to attention-based models.\n",
    "\n",
    "3. **Memory Bottleneck**: Despite improvements over vanilla RNNs, LSTMs still compress all historical information into fixed-size hidden and cell states, potentially losing important details in very long sequences.\n",
    "\n",
    "4. **Training Stability**: While more stable than RNNs, LSTMs can still suffer from exploding gradients and require careful hyperparameter tuning for optimal performance.\n",
    "\n",
    "5. **Limited Bidirectional Context**: In autoregressive settings, LSTMs can only access past context, unlike attention mechanisms that can theoretically access all positions simultaneously.\n",
    "\n",
    "### Application in Language Modeling\n",
    "\n",
    "In autoregressive language modeling, LSTMs process text sequentially, using their internal memory to maintain context about previously seen tokens. The model predicts the next token based on the current hidden state, which encodes information from the entire sequence history. This makes LSTMs particularly suitable for tasks requiring understanding of temporal dependencies and sequential patterns in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed1f7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: (batch, context_size)\n",
    "        embeds = self.embedding(x)  # (batch, context_size, embed_dim)\n",
    "        out, hidden = self.lstm(embeds, hidden)  # out: (batch, context_size, hidden_dim)\n",
    "        out = self.fc(out[:, -1, :])  # usamos solo la última salida\n",
    "        return out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "459e08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 10\n",
    "dataset = NextTokenDataset(encoded_text, context_size)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "vocab_size = max(encoded_text) + 1\n",
    "model = LSTMLanguageModel(vocab_size, embed_dim=64, hidden_dim=128)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad3324fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.0648\n",
      "Epoch 2, Loss: 4.5723\n",
      "Epoch 3, Loss: 4.3101\n",
      "Epoch 4, Loss: 4.0818\n",
      "Epoch 5, Loss: 3.8776\n",
      "Epoch 6, Loss: 3.6875\n",
      "Epoch 7, Loss: 3.5006\n",
      "Epoch 8, Loss: 3.3148\n",
      "Epoch 9, Loss: 3.1320\n",
      "Epoch 10, Loss: 2.9457\n",
      "Epoch 11, Loss: 2.7620\n",
      "Epoch 12, Loss: 2.5796\n",
      "Epoch 13, Loss: 2.3996\n",
      "Epoch 14, Loss: 2.2213\n",
      "Epoch 15, Loss: 2.0430\n",
      "Epoch 16, Loss: 1.8739\n",
      "Epoch 17, Loss: 1.7103\n",
      "Epoch 18, Loss: 1.5497\n",
      "Epoch 19, Loss: 1.4035\n",
      "Epoch 20, Loss: 1.2624\n",
      "Epoch 21, Loss: 1.1279\n",
      "Epoch 22, Loss: 1.0028\n",
      "Epoch 23, Loss: 0.8884\n",
      "Epoch 24, Loss: 0.7787\n",
      "Epoch 25, Loss: 0.6843\n",
      "Epoch 26, Loss: 0.5942\n",
      "Epoch 27, Loss: 0.5136\n",
      "Epoch 28, Loss: 0.4417\n",
      "Epoch 29, Loss: 0.3765\n",
      "Epoch 30, Loss: 0.3194\n",
      "Epoch 31, Loss: 0.2702\n",
      "Epoch 32, Loss: 0.2312\n",
      "Epoch 33, Loss: 0.1904\n",
      "Epoch 34, Loss: 0.1584\n",
      "Epoch 35, Loss: 0.1316\n",
      "Epoch 36, Loss: 0.1096\n",
      "Epoch 37, Loss: 0.0955\n",
      "Epoch 38, Loss: 0.0776\n",
      "Epoch 39, Loss: 0.0610\n",
      "Epoch 40, Loss: 0.0488\n",
      "Epoch 41, Loss: 0.0394\n",
      "Epoch 42, Loss: 0.0330\n",
      "Epoch 43, Loss: 0.0485\n",
      "Epoch 44, Loss: 0.4140\n",
      "Epoch 45, Loss: 0.1154\n",
      "Epoch 46, Loss: 0.0431\n",
      "Epoch 47, Loss: 0.0274\n",
      "Epoch 48, Loss: 0.0217\n",
      "Epoch 49, Loss: 0.0182\n",
      "Epoch 50, Loss: 0.0157\n",
      "Epoch 51, Loss: 0.0136\n",
      "Epoch 52, Loss: 0.0119\n",
      "Epoch 53, Loss: 0.0104\n",
      "Epoch 54, Loss: 0.0091\n",
      "Epoch 55, Loss: 0.0080\n",
      "Epoch 56, Loss: 0.0070\n",
      "Epoch 57, Loss: 0.0061\n",
      "Epoch 58, Loss: 0.0053\n",
      "Epoch 59, Loss: 0.0046\n",
      "Epoch 60, Loss: 0.0040\n",
      "Epoch 61, Loss: 0.0035\n",
      "Epoch 62, Loss: 0.0030\n",
      "Epoch 63, Loss: 0.0180\n",
      "Epoch 64, Loss: 0.6120\n",
      "Epoch 65, Loss: 0.1056\n",
      "Epoch 66, Loss: 0.0314\n",
      "Epoch 67, Loss: 0.0157\n",
      "Epoch 68, Loss: 0.0115\n",
      "Epoch 69, Loss: 0.0094\n",
      "Epoch 70, Loss: 0.0080\n",
      "Epoch 71, Loss: 0.0069\n",
      "Epoch 72, Loss: 0.0060\n",
      "Epoch 73, Loss: 0.0052\n",
      "Epoch 74, Loss: 0.0046\n",
      "Epoch 75, Loss: 0.0040\n",
      "Epoch 76, Loss: 0.0035\n",
      "Epoch 77, Loss: 0.0031\n",
      "Epoch 78, Loss: 0.0027\n",
      "Epoch 79, Loss: 0.0024\n",
      "Epoch 80, Loss: 0.0021\n",
      "Epoch 81, Loss: 0.0019\n",
      "Epoch 82, Loss: 0.0016\n",
      "Epoch 83, Loss: 0.0014\n",
      "Epoch 84, Loss: 0.0012\n",
      "Epoch 85, Loss: 0.0011\n",
      "Epoch 86, Loss: 0.0010\n",
      "Epoch 87, Loss: 0.0008\n",
      "Epoch 88, Loss: 0.0007\n",
      "Epoch 89, Loss: 0.1584\n",
      "Epoch 90, Loss: 0.4365\n",
      "Epoch 91, Loss: 0.0797\n",
      "Epoch 92, Loss: 0.0225\n",
      "Epoch 93, Loss: 0.0108\n",
      "Epoch 94, Loss: 0.0071\n",
      "Epoch 95, Loss: 0.0056\n",
      "Epoch 96, Loss: 0.0047\n",
      "Epoch 97, Loss: 0.0041\n",
      "Epoch 98, Loss: 0.0035\n",
      "Epoch 99, Loss: 0.0031\n",
      "Epoch 100, Loss: 0.0027\n",
      "Epoch 101, Loss: 0.0024\n",
      "Epoch 102, Loss: 0.0021\n",
      "Epoch 103, Loss: 0.0018\n",
      "Epoch 104, Loss: 0.0016\n",
      "Epoch 105, Loss: 0.0014\n",
      "Epoch 106, Loss: 0.0013\n",
      "Epoch 107, Loss: 0.0011\n",
      "Epoch 108, Loss: 0.0010\n",
      "Epoch 109, Loss: 0.0009\n",
      "Epoch 110, Loss: 0.0008\n",
      "Epoch 111, Loss: 0.0007\n",
      "Epoch 112, Loss: 0.0006\n",
      "Epoch 113, Loss: 0.0005\n",
      "Epoch 114, Loss: 0.0004\n",
      "Epoch 115, Loss: 0.0004\n",
      "Epoch 116, Loss: 0.0003\n",
      "Epoch 117, Loss: 0.0003\n",
      "Epoch 118, Loss: 0.0003\n",
      "Epoch 119, Loss: 0.0002\n",
      "Epoch 120, Loss: 0.0002\n",
      "Epoch 121, Loss: 0.4781\n",
      "Epoch 122, Loss: 0.1764\n",
      "Epoch 123, Loss: 0.0394\n",
      "Epoch 124, Loss: 0.0130\n",
      "Epoch 125, Loss: 0.0064\n",
      "Epoch 126, Loss: 0.0046\n",
      "Epoch 127, Loss: 0.0038\n",
      "Epoch 128, Loss: 0.0032\n",
      "Epoch 129, Loss: 0.0028\n",
      "Epoch 130, Loss: 0.0024\n",
      "Epoch 131, Loss: 0.0021\n",
      "Epoch 132, Loss: 0.0018\n",
      "Epoch 133, Loss: 0.0016\n",
      "Epoch 134, Loss: 0.0014\n",
      "Epoch 135, Loss: 0.0012\n",
      "Epoch 136, Loss: 0.0011\n",
      "Epoch 137, Loss: 0.0010\n",
      "Epoch 138, Loss: 0.0008\n",
      "Epoch 139, Loss: 0.0007\n",
      "Epoch 140, Loss: 0.0006\n",
      "Epoch 141, Loss: 0.0006\n",
      "Epoch 142, Loss: 0.0005\n",
      "Epoch 143, Loss: 0.0004\n",
      "Epoch 144, Loss: 0.0004\n",
      "Epoch 145, Loss: 0.0003\n",
      "Epoch 146, Loss: 0.0003\n",
      "Epoch 147, Loss: 0.0003\n",
      "Epoch 148, Loss: 0.0002\n",
      "Epoch 149, Loss: 0.0002\n",
      "Epoch 150, Loss: 0.0002\n",
      "Epoch 151, Loss: 0.0001\n",
      "Epoch 152, Loss: 0.0001\n",
      "Epoch 153, Loss: 0.0001\n",
      "Epoch 154, Loss: 0.0001\n",
      "Epoch 155, Loss: 0.0001\n",
      "Epoch 156, Loss: 0.0001\n",
      "Epoch 157, Loss: 0.2444\n",
      "Epoch 158, Loss: 0.3817\n",
      "Epoch 159, Loss: 0.0695\n",
      "Epoch 160, Loss: 0.0166\n",
      "Epoch 161, Loss: 0.0060\n",
      "Epoch 162, Loss: 0.0040\n",
      "Epoch 163, Loss: 0.0032\n",
      "Epoch 164, Loss: 0.0027\n",
      "Epoch 165, Loss: 0.0023\n",
      "Epoch 166, Loss: 0.0020\n",
      "Epoch 167, Loss: 0.0017\n",
      "Epoch 168, Loss: 0.0015\n",
      "Epoch 169, Loss: 0.0013\n",
      "Epoch 170, Loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(170):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42c005e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(model, encoded_base_text, length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c0355b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en un lugar de la mancha tan de la risa , <UNK> y él tenía que , <UNK> del más <UNK> y <UNK> que se <UNK> <UNK> de rodillas ante mi dulce de sus manos . <UNK> , yo los ojos al señor , y , en efeto , y todos los años , y al\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(generated_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efebe0",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "The Transformer architecture, introduced in \"Attention Is All You Need\" (Vaswani et al., 2017), revolutionized natural language processing by replacing recurrent and convolutional layers with self-attention mechanisms. This architecture has become the foundation for modern large language models like GPT, BERT, and their variants.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "**1. Self-Attention Mechanism**\n",
    "\n",
    "The heart of the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different positions in the input sequence when processing each token. For each position, the model computes attention weights for all other positions, enabling it to capture long-range dependencies effectively.\n",
    "\n",
    "The attention mechanism works through three key components:\n",
    "- **Query (Q)**: What information are we looking for?\n",
    "- **Key (K)**: What information is available at each position?\n",
    "- **Value (V)**: The actual information content at each position\n",
    "\n",
    "The attention score is computed as: Attention(Q,K,V) = softmax(QK^T / √d_k)V\n",
    "\n",
    "**2. Multi-Head Attention**\n",
    "\n",
    "Instead of using a single attention mechanism, Transformers use multiple \"heads\" that can focus on different types of relationships simultaneously. This allows the model to attend to information from different representation subspaces at different positions.\n",
    "\n",
    "**3. Positional Encoding**\n",
    "\n",
    "Since Transformers lack inherent sequential processing, positional encodings are added to input embeddings to provide information about token positions in the sequence.\n",
    "\n",
    "**4. Feed-Forward Networks**\n",
    "\n",
    "Each layer contains a position-wise feed-forward network that processes each position independently, adding non-linearity to the model.\n",
    "\n",
    "### Advantages of Transformers\n",
    "\n",
    "**Parallelization**: Unlike RNNs, all positions can be processed simultaneously during training, leading to significant speedup on modern hardware.\n",
    "\n",
    "**Long-range Dependencies**: Self-attention can directly connect distant positions without the degradation that occurs in RNNs over long sequences.\n",
    "\n",
    "**Interpretability**: Attention weights provide insight into which parts of the input the model is focusing on for each prediction.\n",
    "\n",
    "**Scalability**: Transformers scale exceptionally well with data and compute, following predictable scaling laws.\n",
    "\n",
    "**Transfer Learning**: Pre-trained Transformer models can be fine-tuned for various downstream tasks with remarkable success.\n",
    "\n",
    "### Disadvantages of Transformers\n",
    "\n",
    "**Quadratic Complexity**: Self-attention has O(n²) complexity with respect to sequence length, making it computationally expensive for very long sequences.\n",
    "\n",
    "**Memory Requirements**: The attention mechanism requires storing all pairwise relationships, leading to high memory consumption.\n",
    "\n",
    "**Data Hunger**: Transformers typically require large amounts of training data to achieve good performance.\n",
    "\n",
    "**Lack of Inductive Bias**: Unlike CNNs (translation invariance) or RNNs (sequential bias), Transformers have minimal inductive bias, requiring more data to learn patterns.\n",
    "\n",
    "### Scaling Laws\n",
    "\n",
    "Recent research has revealed predictable scaling laws for Transformer-based language models, showing that performance scales as a power law with:\n",
    "\n",
    "**Model Size (N)**: Larger models with more parameters consistently perform better, following approximately: Loss ∝ N^(-0.076)\n",
    "\n",
    "**Dataset Size (D)**: More training data leads to better performance: Loss ∝ D^(-0.095)\n",
    "\n",
    "**Compute Budget (C)**: More compute during training improves performance: Loss ∝ C^(-0.050)\n",
    "\n",
    "**Key Insights from Scaling Laws:**\n",
    "\n",
    "1. **No Saturation**: Performance continues to improve with scale across several orders of magnitude\n",
    "2. **Predictability**: Performance can be extrapolated from smaller experiments\n",
    "3. **Optimal Allocation**: For a fixed compute budget, there's an optimal trade-off between model size and training time\n",
    "4. **Emergence**: New capabilities often emerge suddenly at certain scales\n",
    "\n",
    "**Chinchilla Scaling Laws**: Recent work suggests that many large models are undertrained relative to their parameter count, and that compute should be split more evenly between model size and training data.\n",
    "\n",
    "### Attention Mechanism Deep Dive\n",
    "\n",
    "The attention mechanism is the core innovation that makes Transformers so effective. Let's examine it in detail:\n",
    "\n",
    "**Scaled Dot-Product Attention**:\n",
    "1. **Linear Projections**: Input embeddings are linearly projected to create Q, K, and V matrices\n",
    "2. **Similarity Computation**: Dot products between queries and keys measure similarity\n",
    "3. **Scaling**: Division by √d_k prevents saturation of the softmax function\n",
    "4. **Normalization**: Softmax ensures attention weights sum to 1\n",
    "5. **Weighted Sum**: Values are weighted by attention scores\n",
    "\n",
    "**Causal Masking**: In autoregressive models, future tokens are masked to prevent information leakage during training.\n",
    "\n",
    "**Multi-Head Benefits**:\n",
    "- Different heads can specialize in different types of relationships (syntactic, semantic, positional)\n",
    "- Increases model capacity without significantly increasing computational cost\n",
    "- Provides robustness through redundancy\n",
    "\n",
    "The attention mechanism's ability to directly model relationships between any two positions in a sequence, regardless of their distance, makes it particularly powerful for language modeling tasks where long-range dependencies are crucial for understanding context and generating coherent text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78449dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=2, context_size=16, ff_dim=256):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(context_size, embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        pos = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        x = self.embedding(x) + self.pos_embedding(pos)  # (B, T, D)\n",
    "\n",
    "        # Máscara causal para autoregresión\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "        \n",
    "        out = self.transformer(x, mask=mask)  # (B, T, D)\n",
    "        out = self.fc(out[:, -1, :])  # solo la última posición\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "91de8cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.1043\n",
      "Epoch 2, Loss: 4.6217\n",
      "Epoch 3, Loss: 4.3471\n",
      "Epoch 4, Loss: 4.1053\n",
      "Epoch 5, Loss: 3.8966\n",
      "Epoch 6, Loss: 3.7043\n",
      "Epoch 7, Loss: 3.5023\n",
      "Epoch 8, Loss: 3.3121\n",
      "Epoch 9, Loss: 3.1129\n",
      "Epoch 10, Loss: 2.9223\n",
      "Epoch 11, Loss: 2.7372\n",
      "Epoch 12, Loss: 2.5459\n",
      "Epoch 13, Loss: 2.3695\n",
      "Epoch 14, Loss: 2.2048\n",
      "Epoch 15, Loss: 2.0313\n",
      "Epoch 16, Loss: 1.8851\n",
      "Epoch 17, Loss: 1.7263\n",
      "Epoch 18, Loss: 1.5891\n",
      "Epoch 19, Loss: 1.4616\n",
      "Epoch 20, Loss: 1.3533\n",
      "Epoch 21, Loss: 1.2237\n",
      "Epoch 22, Loss: 1.1283\n",
      "Epoch 23, Loss: 1.0350\n",
      "Epoch 24, Loss: 0.9558\n",
      "Epoch 25, Loss: 0.8799\n",
      "Epoch 26, Loss: 0.8157\n",
      "Epoch 27, Loss: 0.7320\n",
      "Epoch 28, Loss: 0.6889\n",
      "Epoch 29, Loss: 0.6525\n",
      "Epoch 30, Loss: 0.6072\n",
      "Epoch 31, Loss: 0.5654\n",
      "Epoch 32, Loss: 0.5447\n",
      "Epoch 33, Loss: 0.4979\n",
      "Epoch 34, Loss: 0.4868\n",
      "Epoch 35, Loss: 0.4406\n",
      "Epoch 36, Loss: 0.4357\n",
      "Epoch 37, Loss: 0.4102\n",
      "Epoch 38, Loss: 0.3821\n",
      "Epoch 39, Loss: 0.3639\n",
      "Epoch 40, Loss: 0.3613\n",
      "Epoch 41, Loss: 0.3407\n",
      "Epoch 42, Loss: 0.3393\n",
      "Epoch 43, Loss: 0.3270\n",
      "Epoch 44, Loss: 0.3073\n",
      "Epoch 45, Loss: 0.3001\n",
      "Epoch 46, Loss: 0.2796\n",
      "Epoch 47, Loss: 0.2922\n",
      "Epoch 48, Loss: 0.2728\n",
      "Epoch 49, Loss: 0.2621\n",
      "Epoch 50, Loss: 0.2695\n",
      "Epoch 51, Loss: 0.2720\n",
      "Epoch 52, Loss: 0.2669\n",
      "Epoch 53, Loss: 0.2382\n",
      "Epoch 54, Loss: 0.2312\n",
      "Epoch 55, Loss: 0.2350\n",
      "Epoch 56, Loss: 0.2281\n",
      "Epoch 57, Loss: 0.2218\n",
      "Epoch 58, Loss: 0.2240\n",
      "Epoch 59, Loss: 0.1939\n",
      "Epoch 60, Loss: 0.1974\n",
      "Epoch 61, Loss: 0.2148\n",
      "Epoch 62, Loss: 0.2089\n",
      "Epoch 63, Loss: 0.1945\n",
      "Epoch 64, Loss: 0.1859\n",
      "Epoch 65, Loss: 0.1989\n",
      "Epoch 66, Loss: 0.1845\n",
      "Epoch 67, Loss: 0.1800\n",
      "Epoch 68, Loss: 0.1757\n",
      "Epoch 69, Loss: 0.1825\n",
      "Epoch 70, Loss: 0.1767\n",
      "Epoch 71, Loss: 0.1681\n",
      "Epoch 72, Loss: 0.1758\n",
      "Epoch 73, Loss: 0.1856\n",
      "Epoch 74, Loss: 0.1683\n",
      "Epoch 75, Loss: 0.1639\n",
      "Epoch 76, Loss: 0.1666\n",
      "Epoch 77, Loss: 0.1691\n",
      "Epoch 78, Loss: 0.1523\n",
      "Epoch 79, Loss: 0.1517\n",
      "Epoch 80, Loss: 0.1733\n",
      "Epoch 81, Loss: 0.1469\n",
      "Epoch 82, Loss: 0.1444\n",
      "Epoch 83, Loss: 0.1525\n",
      "Epoch 84, Loss: 0.1518\n",
      "Epoch 85, Loss: 0.1510\n",
      "Epoch 86, Loss: 0.1457\n",
      "Epoch 87, Loss: 0.1488\n",
      "Epoch 88, Loss: 0.1486\n",
      "Epoch 89, Loss: 0.1407\n",
      "Epoch 90, Loss: 0.1382\n",
      "Epoch 91, Loss: 0.1368\n",
      "Epoch 92, Loss: 0.1246\n",
      "Epoch 93, Loss: 0.1310\n",
      "Epoch 94, Loss: 0.1320\n",
      "Epoch 95, Loss: 0.1432\n",
      "Epoch 96, Loss: 0.1281\n",
      "Epoch 97, Loss: 0.1363\n",
      "Epoch 98, Loss: 0.1250\n",
      "Epoch 99, Loss: 0.1192\n",
      "Epoch 100, Loss: 0.1336\n"
     ]
    }
   ],
   "source": [
    "context_size = 16\n",
    "dataset = NextTokenDataset(encoded_text, context_size)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "vocab_size = max(encoded_text) + 1\n",
    "model = TransformerLanguageModel(vocab_size, context_size=context_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "final_loss=[]\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    final_loss.append(total_loss/len(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "882d979b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en un lugar de la mancha dejó mandado . añadió esa gusto : <UNK> vuestra grandeza <UNK> . <UNK> su huésped . <UNK> las prevenciones tan necesarias que había de llevar consigo , <UNK> la de los libros , no le pareció que algún que llegaba a leer por el <UNK> de haberle oído <UNK> lo\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(model, seed_tokens, length=20):\n",
    "    model.eval()\n",
    "    generated = seed_tokens[:]\n",
    "\n",
    "    for _ in range(length):\n",
    "        context = generated[-context_size:]\n",
    "        x = torch.tensor([context], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(next_token)\n",
    "    return generated\n",
    "\n",
    "generated_text = generate(model, encoded_base_text, length=50)\n",
    "decoded_text = tokenizer.decode(generated_text)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f27cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b1f45b9",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "## Fundamental Papers\n",
    "\n",
    "### Transformers and Attention\n",
    "- **Attention Is All You Need** (Vaswani et al., 2017)\n",
    "  - Introduces the Transformer architecture and self-attention mechanism\n",
    "  - https://arxiv.org/abs/1706.03762\n",
    "\n",
    "- **The Annotated Transformer** (Rush, 2018)\n",
    "  - Detailed implementation and explanation of the original paper\n",
    "  - http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "\n",
    "### Autoregressive Language Models\n",
    "- **Language Models are Unsupervised Multitask Learners** (Radford et al., 2019)\n",
    "  - Introduces GPT-2 and demonstrates emergent capabilities in large models\n",
    "  - https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "- **Language Models are Few-Shot Learners** (Brown et al., 2020)\n",
    "  - GPT-3 and demonstration of few-shot learning\n",
    "  - https://arxiv.org/abs/2005.14165\n",
    "\n",
    "### Scaling Laws\n",
    "- **Scaling Laws for Neural Language Models** (Kaplan et al., 2020)\n",
    "  - Establishes fundamental relationships between model size, dataset size, and performance\n",
    "  - https://arxiv.org/abs/2001.08361\n",
    "\n",
    "- **Training Compute-Optimal Large Language Models** (Hoffmann et al., 2022)\n",
    "  - Chinchilla paper: optimal allocation of compute between model size and training tokens\n",
    "  - https://arxiv.org/abs/2203.15556\n",
    "\n",
    "- **PaLM: Scaling Language Modeling with Pathways** (Chowdhery et al., 2022)\n",
    "  - Demonstrates continued scaling benefits up to 540B parameters\n",
    "  - https://arxiv.org/abs/2204.02311\n",
    "\n",
    "- **Emergent Abilities of Large Language Models** (Wei et al., 2022)\n",
    "  - Studies how capabilities emerge at certain scales\n",
    "  - https://arxiv.org/abs/2206.07682\n",
    "\n",
    "### Tokenization and Text Processing\n",
    "- **Neural Machine Translation of Rare Words with Subword Units** (Sennrich et al., 2016)\n",
    "  - Introduces Byte Pair Encoding (BPE)\n",
    "  - https://arxiv.org/abs/1508.07909\n",
    "\n",
    "- **SentencePiece: A simple and language independent subword tokenizer** (Kudo & Richardson, 2018)\n",
    "  - Unified tokenization algorithm\n",
    "  - https://arxiv.org/abs/1808.06226\n",
    "\n",
    "### Optimization and Training\n",
    "- **Deep Residual Learning for Image Recognition** (He et al., 2016)\n",
    "  - Introduces residual connections, fundamental in Transformers\n",
    "  - https://arxiv.org/abs/1512.03385\n",
    "\n",
    "- **Layer Normalization** (Ba et al., 2016)\n",
    "  - Normalization technique used in Transformers\n",
    "  - https://arxiv.org/abs/1607.06450\n",
    "\n",
    "- **Adam: A Method for Stochastic Optimization** (Kingma & Ba, 2014)\n",
    "  - Popular optimizer for training neural networks\n",
    "  - https://arxiv.org/abs/1412.6980\n",
    "\n",
    "### Evaluation and Metrics\n",
    "- **BLEU: a Method for Automatic Evaluation of Machine Translation** (Papineni et al., 2002)\n",
    "  - Standard metric for text generation evaluation\n",
    "  - https://aclanthology.org/P02-1040/\n",
    "\n",
    "- **Perplexity** - Standard metric for language model evaluation\n",
    "  - Measures how well a model predicts a sample\n",
    "\n",
    "## Additional Resources\n",
    "- **The Illustrated Transformer** (Alammar, 2018)\n",
    "  - Visual explanation of Transformer architecture\n",
    "  - http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "- **Hugging Face Transformers Documentation**\n",
    "  - Library and documentation for language models\n",
    "  - https://huggingface.co/docs/transformers/\n",
    "\n",
    "- **OpenAI Scaling Laws Interactive Calculator**\n",
    "  - Tool for exploring scaling relationships\n",
    "  - https://github.com/openai/scaling_laws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb090b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
