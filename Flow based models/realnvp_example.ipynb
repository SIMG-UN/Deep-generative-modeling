{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**\n",
    "\n",
    "The presented code is not optimized, it serves an educational purpose. It is written for CPU, it uses only fully-connected networks and an extremely simplistic dataset. However, it contains all components that can help to understand how RealNVP works, and it should be rather easy to extend it to more sophisticated models. This code could be run almost on any laptop/PC, and it takes a couple of minutes top to get the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we go wild and use a dataset that is simpler than MNIST! We use a scipy dataset called Digits. It consists of ~1500 images of size 8x8, and each pixel can take values in $\\{0, 1, \\ldots, 16\\}$.\n",
    "\n",
    "The goal of using this dataset is that everyone can run it on a laptop, without any gpu etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "        elif mode == 'val':\n",
    "            self.data = digits.data[1000:1350].astype(np.float32)\n",
    "        else:\n",
    "            self.data = digits.data[1350:].astype(np.float32)\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RealNVP code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the blogpost for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, nets, nett, num_flows, prior, D=2, dequantization=True):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        print('RealNVP by JT.')\n",
    "        \n",
    "        self.dequantization = dequantization\n",
    "        \n",
    "        self.prior = prior\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(num_flows)])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(num_flows)])\n",
    "        self.num_flows = num_flows\n",
    "        \n",
    "        self.D = D\n",
    "\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        # x: input, either images (for the first transformation) or outputs from the previous transformation\n",
    "        # index: it determines the index of the transformation\n",
    "        # forward: whether it is a pass from x to y (forward=True), or from y to x (forward=False)\n",
    "        \n",
    "        (xa, xb) = torch.chunk(x, 2, 1)\n",
    "        \n",
    "        s = self.s[index](xa)\n",
    "        t = self.t[index](xa)\n",
    "        \n",
    "        if forward:\n",
    "            #yb = f^{-1}(x)\n",
    "            yb = (xb - t) * torch.exp(-s)\n",
    "        else:\n",
    "            #xb = f(y)\n",
    "            yb = torch.exp(s) * xb + t\n",
    "        \n",
    "        return torch.cat((xa, yb), 1), s\n",
    "\n",
    "    def permute(self, x):\n",
    "        return x.flip(1)\n",
    "\n",
    "    def f(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in range(self.num_flows):\n",
    "            z, s = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "            log_det_J = log_det_J - s.sum(dim=1)\n",
    "\n",
    "        return z, log_det_J\n",
    "\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            x, _ = self.coupling(x, i, forward=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        z, log_det_J = self.f(x)\n",
    "        if reduction == 'sum':\n",
    "            return -(self.prior.log_prob(z) + log_det_J).sum()\n",
    "        else:\n",
    "            return -(self.prior.log_prob(z) + log_det_J).mean()\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        z = self.prior.sample((batchSize, self.D))\n",
    "        z = z[:, 0, :]\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(-1, self.D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions: training, evaluation, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rather self-explanatory, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model',weights_only=False)\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, test_batch in enumerate(test_loader):\n",
    "        if hasattr(model, 'dequantization'):\n",
    "            if model.dequantization:\n",
    "                test_batch = test_batch + (1. - torch.rand(test_batch.shape))/2.\n",
    "        loss_t = model_best.forward(test_batch, reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def samples_real(name, test_loader):\n",
    "    # REAL-------\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = next(iter(test_loader)).detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def samples_generated(name, data_loader, extra_name=''):\n",
    "    x = next(iter(data_loader)).detach().numpy()\n",
    "\n",
    "    # GENERATIONS-------\n",
    "    model_best = torch.load(name + '.model',weights_only=False)\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = model_best.sample(num_x * num_y)\n",
    "    x = x.detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_curve(name, nll_val):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('nll')\n",
    "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    nll_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, batch in enumerate(training_loader):\n",
    "            if hasattr(model, 'dequantization'):\n",
    "                if model.dequantization:\n",
    "                    batch = batch + (1. - torch.rand(batch.shape))/2.\n",
    "            loss = model.forward(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "        else:\n",
    "            if loss_val < best_nll:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                best_nll = loss_val\n",
    "                patience = 0\n",
    "\n",
    "                samples_generated(name, val_loader, extra_name=\"_epoch_\" + str(e))\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "\n",
    "    return nll_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Digits(mode='train')\n",
    "val_data = Digits(mode='val')\n",
    "test_data = Digits(mode='test')\n",
    "\n",
    "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "result_dir = 'results/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "name = 'realnvp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64   # input dimension\n",
    "M = 256  # the number of neurons in scale (s) and translation (t) nets\n",
    "\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 1000 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize RealNVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealNVP by JT.\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "RealNVP                                  --                        --\n",
      "├─ModuleList: 1-15                       --                        (recursive)\n",
      "│    └─Sequential: 2-1                   [1, 32]                   --\n",
      "│    │    └─Linear: 3-1                  [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-2               [1, 256]                  --\n",
      "│    │    └─Linear: 3-3                  [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-4               [1, 256]                  --\n",
      "│    │    └─Linear: 3-5                  [1, 32]                   8,224\n",
      "│    │    └─Tanh: 3-6                    [1, 32]                   --\n",
      "├─ModuleList: 1-16                       --                        (recursive)\n",
      "│    └─Sequential: 2-2                   [1, 32]                   --\n",
      "│    │    └─Linear: 3-7                  [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-8               [1, 256]                  --\n",
      "│    │    └─Linear: 3-9                  [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-10              [1, 256]                  --\n",
      "│    │    └─Linear: 3-11                 [1, 32]                   8,224\n",
      "├─ModuleList: 1-15                       --                        (recursive)\n",
      "│    └─Sequential: 2-3                   [1, 32]                   --\n",
      "│    │    └─Linear: 3-12                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-13              [1, 256]                  --\n",
      "│    │    └─Linear: 3-14                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-15              [1, 256]                  --\n",
      "│    │    └─Linear: 3-16                 [1, 32]                   8,224\n",
      "│    │    └─Tanh: 3-17                   [1, 32]                   --\n",
      "├─ModuleList: 1-16                       --                        (recursive)\n",
      "│    └─Sequential: 2-4                   [1, 32]                   --\n",
      "│    │    └─Linear: 3-18                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-19              [1, 256]                  --\n",
      "│    │    └─Linear: 3-20                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-21              [1, 256]                  --\n",
      "│    │    └─Linear: 3-22                 [1, 32]                   8,224\n",
      "├─ModuleList: 1-15                       --                        (recursive)\n",
      "│    └─Sequential: 2-5                   [1, 32]                   --\n",
      "│    │    └─Linear: 3-23                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-24              [1, 256]                  --\n",
      "│    │    └─Linear: 3-25                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-26              [1, 256]                  --\n",
      "│    │    └─Linear: 3-27                 [1, 32]                   8,224\n",
      "│    │    └─Tanh: 3-28                   [1, 32]                   --\n",
      "├─ModuleList: 1-16                       --                        (recursive)\n",
      "│    └─Sequential: 2-6                   [1, 32]                   --\n",
      "│    │    └─Linear: 3-29                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-30              [1, 256]                  --\n",
      "│    │    └─Linear: 3-31                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-32              [1, 256]                  --\n",
      "│    │    └─Linear: 3-33                 [1, 32]                   8,224\n",
      "├─ModuleList: 1-15                       --                        (recursive)\n",
      "│    └─Sequential: 2-7                   [1, 32]                   --\n",
      "│    │    └─Linear: 3-34                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-35              [1, 256]                  --\n",
      "│    │    └─Linear: 3-36                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-37              [1, 256]                  --\n",
      "│    │    └─Linear: 3-38                 [1, 32]                   8,224\n",
      "│    │    └─Tanh: 3-39                   [1, 32]                   --\n",
      "├─ModuleList: 1-16                       --                        (recursive)\n",
      "│    └─Sequential: 2-8                   [1, 32]                   --\n",
      "│    │    └─Linear: 3-40                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-41              [1, 256]                  --\n",
      "│    │    └─Linear: 3-42                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-43              [1, 256]                  --\n",
      "│    │    └─Linear: 3-44                 [1, 32]                   8,224\n",
      "├─ModuleList: 1-15                       --                        (recursive)\n",
      "│    └─Sequential: 2-9                   [1, 32]                   --\n",
      "│    │    └─Linear: 3-45                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-46              [1, 256]                  --\n",
      "│    │    └─Linear: 3-47                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-48              [1, 256]                  --\n",
      "│    │    └─Linear: 3-49                 [1, 32]                   8,224\n",
      "│    │    └─Tanh: 3-50                   [1, 32]                   --\n",
      "├─ModuleList: 1-16                       --                        (recursive)\n",
      "│    └─Sequential: 2-10                  [1, 32]                   --\n",
      "│    │    └─Linear: 3-51                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-52              [1, 256]                  --\n",
      "│    │    └─Linear: 3-53                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-54              [1, 256]                  --\n",
      "│    │    └─Linear: 3-55                 [1, 32]                   8,224\n",
      "├─ModuleList: 1-15                       --                        (recursive)\n",
      "│    └─Sequential: 2-11                  [1, 32]                   --\n",
      "│    │    └─Linear: 3-56                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-57              [1, 256]                  --\n",
      "│    │    └─Linear: 3-58                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-59              [1, 256]                  --\n",
      "│    │    └─Linear: 3-60                 [1, 32]                   8,224\n",
      "│    │    └─Tanh: 3-61                   [1, 32]                   --\n",
      "├─ModuleList: 1-16                       --                        (recursive)\n",
      "│    └─Sequential: 2-12                  [1, 32]                   --\n",
      "│    │    └─Linear: 3-62                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-63              [1, 256]                  --\n",
      "│    │    └─Linear: 3-64                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-65              [1, 256]                  --\n",
      "│    │    └─Linear: 3-66                 [1, 32]                   8,224\n",
      "├─ModuleList: 1-15                       --                        (recursive)\n",
      "│    └─Sequential: 2-13                  [1, 32]                   --\n",
      "│    │    └─Linear: 3-67                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-68              [1, 256]                  --\n",
      "│    │    └─Linear: 3-69                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-70              [1, 256]                  --\n",
      "│    │    └─Linear: 3-71                 [1, 32]                   8,224\n",
      "│    │    └─Tanh: 3-72                   [1, 32]                   --\n",
      "├─ModuleList: 1-16                       --                        (recursive)\n",
      "│    └─Sequential: 2-14                  [1, 32]                   --\n",
      "│    │    └─Linear: 3-73                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-74              [1, 256]                  --\n",
      "│    │    └─Linear: 3-75                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-76              [1, 256]                  --\n",
      "│    │    └─Linear: 3-77                 [1, 32]                   8,224\n",
      "├─ModuleList: 1-15                       --                        (recursive)\n",
      "│    └─Sequential: 2-15                  [1, 32]                   --\n",
      "│    │    └─Linear: 3-78                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-79              [1, 256]                  --\n",
      "│    │    └─Linear: 3-80                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-81              [1, 256]                  --\n",
      "│    │    └─Linear: 3-82                 [1, 32]                   8,224\n",
      "│    │    └─Tanh: 3-83                   [1, 32]                   --\n",
      "├─ModuleList: 1-16                       --                        (recursive)\n",
      "│    └─Sequential: 2-16                  [1, 32]                   --\n",
      "│    │    └─Linear: 3-84                 [1, 256]                  8,448\n",
      "│    │    └─LeakyReLU: 3-85              [1, 256]                  --\n",
      "│    │    └─Linear: 3-86                 [1, 256]                  65,792\n",
      "│    │    └─LeakyReLU: 3-87              [1, 256]                  --\n",
      "│    │    └─Linear: 3-88                 [1, 32]                   8,224\n",
      "==========================================================================================\n",
      "Total params: 1,319,424\n",
      "Trainable params: 1,319,424\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 1.32\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.07\n",
      "Params size (MB): 5.28\n",
      "Estimated Total Size (MB): 5.35\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# The number of invertible transformations\n",
    "num_flows = 8\n",
    "\n",
    "# scale (s) network\n",
    "nets = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2), nn.Tanh())\n",
    "\n",
    "# translation (t) network\n",
    "nett = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2))\n",
    "\n",
    "# Prior (a.k.a. the base distribution): Gaussian\n",
    "prior = torch.distributions.MultivariateNormal(torch.zeros(D), torch.eye(D))\n",
    "# Init RealNVP\n",
    "model = RealNVP(nets, nett, num_flows, prior, D=D, dequantization=True)\n",
    "# Print the summary (like in Keras)\n",
    "# Asegurar que todo esté en CPU para evitar conflictos de dispositivos\n",
    "model_sum = model.cpu()\n",
    "print(summary(model_sum, input_size=(1, 64), device='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play! Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val nll=255.06479073660714\n",
      "saved!\n",
      "Epoch: 1, val nll=216.58772321428572\n",
      "saved!\n",
      "Epoch: 2, val nll=201.98664341517858\n",
      "saved!\n",
      "Epoch: 3, val nll=192.99509486607144\n",
      "saved!\n",
      "Epoch: 4, val nll=186.29610770089286\n",
      "saved!\n",
      "Epoch: 5, val nll=180.44013392857144\n",
      "saved!\n",
      "Epoch: 6, val nll=176.25202845982142\n",
      "saved!\n",
      "Epoch: 7, val nll=172.56055106026787\n",
      "saved!\n",
      "Epoch: 8, val nll=169.7835267857143\n",
      "saved!\n",
      "Epoch: 9, val nll=167.32252511160715\n",
      "saved!\n",
      "Epoch: 10, val nll=165.5539453125\n",
      "saved!\n",
      "Epoch: 11, val nll=163.90202566964285\n",
      "saved!\n",
      "Epoch: 12, val nll=162.38700334821428\n",
      "saved!\n",
      "Epoch: 13, val nll=161.84489815848215\n",
      "saved!\n",
      "Epoch: 14, val nll=160.44199358258928\n",
      "saved!\n",
      "Epoch: 15, val nll=159.61655691964285\n",
      "saved!\n",
      "Epoch: 16, val nll=159.06721261160715\n",
      "saved!\n",
      "Epoch: 17, val nll=157.96448521205357\n",
      "saved!\n",
      "Epoch: 18, val nll=157.47361746651785\n",
      "saved!\n",
      "Epoch: 19, val nll=156.70869419642858\n",
      "saved!\n",
      "Epoch: 20, val nll=156.1651869419643\n",
      "saved!\n",
      "Epoch: 21, val nll=156.18666294642858\n",
      "Epoch: 22, val nll=155.69849330357144\n",
      "saved!\n",
      "Epoch: 23, val nll=155.23468191964287\n",
      "saved!\n",
      "Epoch: 24, val nll=154.93988978794644\n",
      "saved!\n",
      "Epoch: 25, val nll=154.47988839285713\n",
      "saved!\n",
      "Epoch: 26, val nll=154.30411551339284\n",
      "saved!\n",
      "Epoch: 27, val nll=154.66161830357143\n",
      "Epoch: 28, val nll=154.5248939732143\n",
      "Epoch: 29, val nll=153.61365931919642\n",
      "saved!\n",
      "Epoch: 30, val nll=152.64386997767858\n",
      "saved!\n",
      "Epoch: 31, val nll=153.10127790178572\n",
      "Epoch: 32, val nll=153.240107421875\n",
      "Epoch: 33, val nll=152.9015443638393\n",
      "Epoch: 34, val nll=152.87857979910714\n",
      "Epoch: 35, val nll=153.482685546875\n",
      "Epoch: 36, val nll=152.7900306919643\n",
      "Epoch: 37, val nll=151.52416434151786\n",
      "saved!\n",
      "Epoch: 38, val nll=151.41041015625\n",
      "saved!\n",
      "Epoch: 39, val nll=150.79665597098213\n",
      "saved!\n",
      "Epoch: 40, val nll=150.89268833705358\n",
      "Epoch: 41, val nll=150.91720424107143\n",
      "Epoch: 42, val nll=151.41597098214285\n",
      "Epoch: 43, val nll=149.85536551339285\n",
      "saved!\n",
      "Epoch: 44, val nll=150.82624162946428\n",
      "Epoch: 45, val nll=151.46828962053573\n",
      "Epoch: 46, val nll=150.0291029575893\n",
      "Epoch: 47, val nll=148.34590541294642\n",
      "saved!\n",
      "Epoch: 48, val nll=149.31701590401786\n",
      "Epoch: 49, val nll=148.70868443080357\n",
      "Epoch: 50, val nll=148.84975864955356\n",
      "Epoch: 51, val nll=148.6298828125\n",
      "Epoch: 52, val nll=147.75723074776786\n",
      "saved!\n",
      "Epoch: 53, val nll=147.28410714285715\n",
      "saved!\n",
      "Epoch: 54, val nll=148.49350864955358\n",
      "Epoch: 55, val nll=147.30049665178572\n",
      "Epoch: 56, val nll=149.39843052455356\n",
      "Epoch: 57, val nll=146.78661830357143\n",
      "saved!\n",
      "Epoch: 58, val nll=146.95497349330358\n",
      "Epoch: 59, val nll=149.53848353794643\n",
      "Epoch: 60, val nll=147.2184849330357\n",
      "Epoch: 61, val nll=147.05117885044643\n",
      "Epoch: 62, val nll=146.97143973214287\n",
      "Epoch: 63, val nll=146.31548130580356\n",
      "saved!\n",
      "Epoch: 64, val nll=146.73786969866072\n",
      "Epoch: 65, val nll=147.75059988839286\n",
      "Epoch: 66, val nll=149.073359375\n",
      "Epoch: 67, val nll=147.36647739955356\n",
      "Epoch: 68, val nll=146.33353515625\n",
      "Epoch: 69, val nll=151.60340959821428\n",
      "Epoch: 70, val nll=145.810390625\n",
      "saved!\n",
      "Epoch: 71, val nll=149.14403180803572\n",
      "Epoch: 72, val nll=146.87299107142857\n",
      "Epoch: 73, val nll=147.73129603794644\n",
      "Epoch: 74, val nll=147.83685128348213\n",
      "Epoch: 75, val nll=148.26662667410713\n",
      "Epoch: 76, val nll=145.98776785714287\n",
      "Epoch: 77, val nll=144.60437081473214\n",
      "saved!\n",
      "Epoch: 78, val nll=147.35039760044643\n",
      "Epoch: 79, val nll=147.0163364955357\n",
      "Epoch: 80, val nll=145.58656110491071\n",
      "Epoch: 81, val nll=144.0114536830357\n",
      "saved!\n",
      "Epoch: 82, val nll=147.212001953125\n",
      "Epoch: 83, val nll=144.74783203125\n",
      "Epoch: 84, val nll=145.09171595982144\n",
      "Epoch: 85, val nll=143.66736746651785\n",
      "saved!\n",
      "Epoch: 86, val nll=143.234248046875\n",
      "saved!\n",
      "Epoch: 87, val nll=144.35207589285713\n",
      "Epoch: 88, val nll=144.49859793526787\n",
      "Epoch: 89, val nll=144.61643136160714\n",
      "Epoch: 90, val nll=142.85298828125\n",
      "saved!\n",
      "Epoch: 91, val nll=139.32490513392858\n",
      "saved!\n",
      "Epoch: 92, val nll=143.37797712053572\n",
      "Epoch: 93, val nll=140.48877511160714\n",
      "Epoch: 94, val nll=141.26210658482142\n",
      "Epoch: 95, val nll=139.5631654575893\n",
      "Epoch: 96, val nll=140.06996512276785\n",
      "Epoch: 97, val nll=144.08505161830357\n",
      "Epoch: 98, val nll=140.00964285714286\n",
      "Epoch: 99, val nll=137.06504603794642\n",
      "saved!\n",
      "Epoch: 100, val nll=138.76501534598214\n",
      "Epoch: 101, val nll=136.33633091517856\n",
      "saved!\n",
      "Epoch: 102, val nll=138.570654296875\n",
      "Epoch: 103, val nll=134.73409737723213\n",
      "saved!\n",
      "Epoch: 104, val nll=136.7882407924107\n",
      "Epoch: 105, val nll=134.21322265625\n",
      "saved!\n",
      "Epoch: 106, val nll=135.46320591517858\n",
      "Epoch: 107, val nll=134.1858579799107\n",
      "saved!\n",
      "Epoch: 108, val nll=135.06306082589285\n",
      "Epoch: 109, val nll=138.01436383928572\n",
      "Epoch: 110, val nll=136.946806640625\n",
      "Epoch: 111, val nll=136.12498744419642\n",
      "Epoch: 112, val nll=133.36525948660713\n",
      "saved!\n",
      "Epoch: 113, val nll=132.4933579799107\n",
      "saved!\n",
      "Epoch: 114, val nll=133.55718470982143\n",
      "Epoch: 115, val nll=135.24605887276786\n",
      "Epoch: 116, val nll=132.81663783482142\n",
      "Epoch: 117, val nll=134.02726143973214\n",
      "Epoch: 118, val nll=133.224716796875\n",
      "Epoch: 119, val nll=135.165595703125\n",
      "Epoch: 120, val nll=132.80714285714285\n",
      "Epoch: 121, val nll=135.62946847098215\n",
      "Epoch: 122, val nll=133.87343470982142\n",
      "Epoch: 123, val nll=133.93006556919642\n",
      "Epoch: 124, val nll=133.7809751674107\n",
      "Epoch: 125, val nll=131.35964146205356\n",
      "saved!\n",
      "Epoch: 126, val nll=131.79273577008928\n",
      "Epoch: 127, val nll=134.87550083705358\n",
      "Epoch: 128, val nll=129.22877511160715\n",
      "saved!\n",
      "Epoch: 129, val nll=132.55498604910716\n",
      "Epoch: 130, val nll=132.14083705357143\n",
      "Epoch: 131, val nll=131.59770368303572\n",
      "Epoch: 132, val nll=130.89408342633928\n",
      "Epoch: 133, val nll=132.63007114955357\n",
      "Epoch: 134, val nll=133.02214425223215\n",
      "Epoch: 135, val nll=130.19459681919642\n",
      "Epoch: 136, val nll=132.08971958705357\n",
      "Epoch: 137, val nll=128.65601283482144\n",
      "saved!\n",
      "Epoch: 138, val nll=128.48029017857144\n",
      "saved!\n",
      "Epoch: 139, val nll=131.76049107142856\n",
      "Epoch: 140, val nll=129.70233537946427\n",
      "Epoch: 141, val nll=132.1598451450893\n",
      "Epoch: 142, val nll=131.3569140625\n",
      "Epoch: 143, val nll=130.73228794642858\n",
      "Epoch: 144, val nll=129.71998744419642\n",
      "Epoch: 145, val nll=130.08486886160713\n",
      "Epoch: 146, val nll=132.82313755580358\n",
      "Epoch: 147, val nll=131.85618722098215\n",
      "Epoch: 148, val nll=131.58287946428572\n",
      "Epoch: 149, val nll=130.08346121651786\n",
      "Epoch: 150, val nll=131.95062779017857\n",
      "Epoch: 151, val nll=132.99096121651786\n",
      "Epoch: 152, val nll=132.21038783482143\n",
      "Epoch: 153, val nll=132.57834681919644\n",
      "Epoch: 154, val nll=132.68362862723214\n",
      "Epoch: 155, val nll=130.538388671875\n",
      "Epoch: 156, val nll=131.1245493861607\n",
      "Epoch: 157, val nll=133.49166294642856\n",
      "Epoch: 158, val nll=132.26445033482142\n",
      "Epoch: 159, val nll=133.5198939732143\n"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "nll_val = training(name=result_dir + name, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
    "                       training_loader=training_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL LOSS: nll=116.91248623636746\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
    "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
    "f.write(str(test_loss))\n",
    "f.close()\n",
    "\n",
    "samples_real(result_dir + name, test_loader)\n",
    "\n",
    "plot_curve(result_dir + name, nll_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
